{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "014ee45e6f4621d236dae339577196c1",
     "grade": false,
     "grade_id": "cell-86838232e118703e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-danger\" style=\"color:black\"><b>Running ML-LV Jupyter Notebooks:</b><br>\n",
    "    <ol>\n",
    "        <li>Make sure you are running all notebooks using the <code>adv_ai</code> kernel.\n",
    "        <li><b>It is very important that you do not create any additional files within the weekly folders on CSCT cloud.</b> Any additional files, or editing the notebooks with a different environment may prevent submission/marking of your work.</li>\n",
    "            <ul>\n",
    "                <li>NBGrader will automatically fetch and create the correct folders files for you.</li>\n",
    "                <li>All files that are not the Jupyter notebooks should be stored in the 'ML-LV/data' directory.</li>\n",
    "            </ul>\n",
    "        <li>Please <b>do not pip install</b> any python packages (or anything else). You should not need to install anything to complete these notebooks other than the packages provided in the Jupyter CSCT Cloud environment.</li>\n",
    "    </ol>\n",
    "    <b>If you would like to run this notebook locally you should:</b><br>\n",
    "    <ol>\n",
    "        <li>Create an environment using the requirements.txt file provided. <b>Any additional packages you install will not be accessible when uploaded to the server and may prevent marking.</b></li>\n",
    "        <li>Download a copy¬† of the notebook to your own machine. You can then edit the cells as you wish and then go back and copy the code into/edit the ones on the CSCT cloud in-place.</li>\n",
    "        <li><b>It is very important that you do not re-upload any notebooks that you have edited locally.</b> This is because NBGrader uses cell metadata to track marked tasks. <b>If you change this format it may prevent marking.</b></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36840370502be34777bbdf0a71f0a9d9",
     "grade": false,
     "grade_id": "cell-e1ea4f7d2cd8d694",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Practical 2: Text Pre-processing and Representation\n",
    "\n",
    "In the previous practical we gathered movie reviews from IMDB and annotated them with sentiment. When data is scraped from the web, or even gathered from other sources, it is unlikely to be in a suitable format for NLP applications. So, now that we have some data, the next step is to clean and normalise the text. This will reduce noise within the data and ensure a consistent set of input features for an ML model. Very frequent or infrequent words, punctuation and other characters, emojis, HTML tags etc all increase the number of features present within the data. Each of these may, or may not, be helpful when training a model for a given task.\n",
    "\n",
    "In the first part of this practical we will examine several text pre-processing and normalisation steps and the process of building a vocabulary. Then develop a function to apply each of these steps to our imdb review data.\n",
    "\n",
    "In the second part of this practical we will look at several different methods of representing text in a format that is compatible with ML models, i.e. as numbers or vectors.\n",
    "\n",
    "The objectives of this practical are:\n",
    "1. Understand various text pre-processing options and determine which are appropriate for a given problem\n",
    "\n",
    "2. Develop text pre-processing and create a vocabulary functions\n",
    "\n",
    "3. Explore vectorised language representations - BOW, One-hot and TF-IDF\n",
    "\n",
    "4. Understand the benefit of word vectors and how to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc334ac164a677aa54afa87beddfe5cb",
     "grade": false,
     "grade_id": "cell-27c40c3a075ba24d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1 Text Pre-processing\n",
    "\n",
    "## 1.0 Import libraries\n",
    "\n",
    "1. [spaCy](https://spacy.io/) - is a Python library for NLP. It's very efficient and has an excellent set of features.\n",
    "\n",
    "2. [Natural Language Toolkit (NLTK)](https://www.nltk.org/) - is an older but more comprehensive NLP toolkit for Python.\n",
    "\n",
    "3. [Unidecode](https://pypi.org/project/Unidecode/) - is a small Python package for stripping accents from letters.\n",
    "\n",
    "4. [Contractions](https://github.com/kootenpv/contractions) - is a small Python package for expanding contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import unidecode\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Get the status of NBgrader (for skipping cell execution while validating/grading)\n",
    "grading = True if os.getenv('NBGRADER_EXECUTION') else False\n",
    "\n",
    "# Get the project directory (should be in ML-LV)\n",
    "path = ''\n",
    "while os.path.basename(os.path.abspath(path)) != 'ML-LV':\n",
    "    path = os.path.abspath(os.path.join(path, '..'))\n",
    "\n",
    "# Set the directory to the data folder (should be in ML-LV/data/imdb)\n",
    "data_dir = os.path.join(path, 'data', 'imdb')\n",
    "\n",
    "# Set the directory to the shared dataset folder (should be in shared/datasets/imdb)\n",
    "dataset_dir = os.path.join(path, '..', 'shared', 'datasets', 'imdb')\n",
    "\n",
    "# Load the Spacy language model ('en_core_web_md' should be in shared/models/spacy)\n",
    "nlp = spacy.load(os.path.join(path, '..', 'shared', 'models', 'spacy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ede10b06eb3793019e27d3b299486356",
     "grade": false,
     "grade_id": "cell-b4185a6ed6b7bddf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Pre-processing options\n",
    "\n",
    "The following cells demonstrate each of the pre-processing options discussed in the lecture. For most we use spaCy but several are possible using regular expressions or plain Python.\n",
    "\n",
    "It is very unlikely, you would ever need to apply **all** of these steps. In fact you probably wouldn't have much text left if you did! But it is important to understand what each does and when they might be appropriate.\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color:black\"><b>Note:</b> The <i>order</i> these steps are applied can sometimes make a big difference.<br>\n",
    "For example, if you were to remove punctuation and replace with an empty string, then hyphenated words would be joined together. So, 'father-in-law' becomes 'fatherinlaw'.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16feb8ca624fe116531f19940d1dd55c",
     "grade": false,
     "grade_id": "cell-d3e4c3601798cefb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Tokenisation and Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Let's visit my father-in-law in St. Louis next year. He said 'it would be fun!'\n",
      "\n",
      "Sentences: [Let's visit my father-in-law in St. Louis next year., He said 'it would be fun!']\n",
      "\n",
      "Tokens: [['Let', \"'s\", 'visit', 'my', 'father', '-', 'in', '-', 'law', 'in', 'St.', 'Louis', 'next', 'year', '.'], ['He', 'said', \"'\", 'it', 'would', 'be', 'fun', '!', \"'\"]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spacy document object\n",
    "raw_text = \"Let's visit my father-in-law in St. Louis next year. He said 'it would be fun!'\"\n",
    "doc = nlp(raw_text)\n",
    "print(f\"Document: {doc}\\n\")\n",
    "\n",
    "# Segment the text into sentences\n",
    "sentences = list([sent for sent in doc.sents])\n",
    "print(f\"Sentences: {sentences}\\n\")\n",
    "\n",
    "# Tokenise the sentences\n",
    "tokens = []\n",
    "for sent in doc.sents:\n",
    "    tokens.append([token.text for token in sent])\n",
    "print(f\"Tokens: {tokens}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c43e2e83c3af6f8476f6b3d57760182",
     "grade": false,
     "grade_id": "cell-0a54ddf1d36e7cc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Stemming and Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:               Stem:                Lemma:              \n",
      "\n",
      "studies              studi                study               \n",
      "studying             studi                study               \n",
      "cries                cri                  cry                 \n",
      "cry                  cri                  cry                 \n",
      "automatic            automat              automatic           \n",
      "automation           autom                automation          \n",
      "are                  are                  be                  \n",
      "is                   is                   be                  \n",
      "car                  car                  car                 \n",
      "cars                 car                  car                 \n",
      "am                   am                   be                  \n"
     ]
    }
   ],
   "source": [
    "# Create NLTk stemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Create spacy document object\n",
    "raw_text = \"studies studying cries cry automatic automation are is car cars am\"\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "# Print the stem and lemma for each token\n",
    "print(f\"{'Token:':20} {'Stem:':20} {'Lemma:':20}\\n\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:20} {stemmer.stem(token.text):20} {token.lemma_:20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0fc59f83d7065a530e5d7ef847592bb5",
     "grade": false,
     "grade_id": "cell-90df7617543e5a7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Stop words, Case-folding and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stop words: ['amongst', '‚Äòll', '‚Äòve', 'had', 'on', 'bottom', 'would', 'whereby', 'after', 'therein', 'every', 'noone', 'became', 're', 'towards', 'hers', 'there', 'thru', 'show', 'itself', 'ten', 'become', 'formerly', 'therefore', 'afterwards', 'because', 'if', 'perhaps', 'whose', 'wherein', 'how', 'latter', 'such', 'somehow', 'four', 'was', 'out', 'twenty', \"'re\", 'across', 'hereby', 'whither', 'last', 'when', 'am', 'that', 'and', 'five', 'than', '‚Äòre'] \n",
      "\n",
      "Document: Let's visit my #father-in-law @ St. Louis next year.\n",
      " He said 'it would be fun!'\n",
      "\n",
      "Removed stop words:\n",
      "[Let, visit, #, father, -, -, law, @, St., Louis, year, ., \n",
      " ]\n",
      "[said, ', fun, !, ']\n",
      "\n",
      "Lower-cased words:\n",
      "['let', \"'s\", 'visit', 'my', '#', 'father', '-', 'in', '-', 'law', '@', 'st.', 'louis', 'next', 'year', '.', '\\n ']\n",
      "['he', 'said', \"'\", 'it', 'would', 'be', 'fun', '!', \"'\"]\n",
      "\n",
      "Removed punctuation:\n",
      "[Let, 's, visit, my, father, in, law, St., Louis, next, year, \n",
      " ]\n",
      "[He, said, it, would, be, fun]\n"
     ]
    }
   ],
   "source": [
    "# Print Spacy's default stop words\n",
    "print(f\"List of stop words: {list(nlp.Defaults.stop_words)[:50]} \\n\")\n",
    "\n",
    "# Create spacy document object\n",
    "raw_text = \"Let's visit my #father-in-law @ St. Louis next year.\\n He said 'it would be fun!'\"\n",
    "doc = nlp(raw_text)\n",
    "print(f\"Document: {doc}\")\n",
    "\n",
    "# Remove stop words\n",
    "print(\"\\nRemoved stop words:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token for token in sent if not token.is_stop]\n",
    "    print(sent)\n",
    "\n",
    "# Lowercase the tokens\n",
    "# Python: text.lower()\n",
    "print(\"\\nLower-cased words:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token.lower_ for token in sent]\n",
    "    print(sent)\n",
    "\n",
    "# Remove punctuation\n",
    "# Regex: keep only letters and numbers\n",
    "# re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "print(\"\\nRemoved punctuation:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token for token in sent if not token.is_punct]\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbd8edb3f01ffeba99d5bc3fe9b5e6db",
     "grade": false,
     "grade_id": "cell-38f0968d85db2e06",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Whitespace, Characters, Contractions, Accents, HTML tags and Emoji\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color:black\"><b>Note:</b> The regex for removing emojis is from <a href=https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python>this stack overflow answer</a>.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\" style=\"color:black\">\n",
    "<b>Parsing HTML with regex:</b> The regular expression used here works reasonably well for simple HTML tags but is not fool proof, as jokingly outlined <a href=https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags?noredirect=1&lq=1>in this well known stack overflow answer</a>.<br>\n",
    "\n",
    "Regex cannot reliably account for the complex structure of HTML, so if it is critical to correctly parse HTML you should use an XML parser or something like Beautiful Soup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: <a href='site.com' class='link'> Let's visit m√Ω \t #f√°ther-in-law @ St. Louis next year.\n",
      " <b>He said 'it would be fun!' üòÇ </b></a><br>\n",
      "\n",
      "Removed whitespace characters:\n",
      "[<, a, href='site.com, ', class='link, ', >, Let, 's, visit, m√Ω, #, f√°ther, -, in, -, law, @, St., Louis, next, year, .]\n",
      "[<, b, >]\n",
      "[He, said, ', it, would, be, fun, !, ']\n",
      "[üòÇ, <, /b></a><br, >]\n",
      "\n",
      "Removed specific characters:\n",
      "<a href='site.com' class='link'> Let's visit m√Ω \t f√°ther-in-law  St. Louis next year.\n",
      " \n",
      "<b>\n",
      "He said 'it would be fun!'\n",
      "üòÇ </b></a><br>\n",
      "\n",
      "Removed accents:\n",
      "<a href='site.com' class='link'> Let's visit my \t #father-in-law @ St. Louis next year.\n",
      " \n",
      "<b>\n",
      "He said 'it would be fun!'\n",
      " </b></a><br>\n",
      "\n",
      "Expanded contractions:\n",
      "<a href='site.com' class='link'> Let us visit m√Ω \t #f√°ther-in-law @ St. Louis next year.\n",
      " \n",
      "<b>\n",
      "He said 'it would be fun!'\n",
      "üòÇ </b></a><br>\n",
      "\n",
      "Removed HTML tags:\n",
      " Let's visit m√Ω \t #f√°ther-in-law @ St. Louis next year.\n",
      " \n",
      "\n",
      "He said 'it would be fun!'\n",
      "üòÇ \n",
      "\n",
      "Removed emoji:\n",
      "<a href='site.com' class='link'> Let's visit m√Ω \t #f√°ther-in-law @ St. Louis next year.\n",
      " \n",
      "<b>\n",
      "He said 'it would be fun!'\n",
      " </b></a><br>\n"
     ]
    }
   ],
   "source": [
    "# Create spacy document object\n",
    "raw_text = u\"<a href='site.com' class='link'> Let's visit m√Ω \\t #f√°ther-in-law @ St. Louis next year.\\n <b>He said 'it would be fun!' \\U0001f602 </b></a><br>\"\n",
    "\n",
    "doc = nlp(raw_text)\n",
    "print(f\"Document: {doc}\")\n",
    "\n",
    "# Remove whitespace\n",
    "# Regex: remove 1 or more whitespace characters\n",
    "# re.sub('\\s+', ' ', text)\n",
    "print(\"\\nRemoved whitespace characters:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token for token in sent if not token.is_space]\n",
    "    print(sent)\n",
    "\n",
    "# Remove specific characters\n",
    "# Characters are specified inside the square brackets\n",
    "print(\"\\nRemoved specific characters:\")\n",
    "for sent in doc.sents:\n",
    "    sent = re.sub('[@#$]', '', sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Remove accents\n",
    "print(\"\\nRemoved accents:\")\n",
    "for sent in doc.sents:\n",
    "    sent = unidecode.unidecode(sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Expand contractions\n",
    "print(\"\\nExpanded contractions:\")\n",
    "for sent in doc.sents:\n",
    "    sent = contractions.fix(sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Remove HTML tags\n",
    "# Match 0 or more characters between < and >\n",
    "print(\"\\nRemoved HTML tags:\")\n",
    "for sent in doc.sents:\n",
    "    sent = re.sub('<.*?>', '', sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Remove emojis\n",
    "emoji_pattern = re.compile(\"[\"u\"\\U0001F100-\\U0001FFFF\"\"]+\", flags=re.UNICODE)\n",
    "print(\"\\nRemoved emoji:\")\n",
    "for sent in doc.sents:\n",
    "    sent = emoji_pattern.sub(r'', sent.text)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f7391be32e93b9d58d92be6f8bdfe9d",
     "grade": false,
     "grade_id": "cell-5b4ef85b98eb7afe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Building a vocabulary\n",
    "\n",
    "It is often helpful to create a vocabulary once the text has been processed. At a certain point words appear so infrequently they may have little impact on the model. So a vocabulary allows us to choose how many words (features) to keep and then discard those that are less frequently occurring.\n",
    "\n",
    "A vocabulary also allows us to map word tokens to indices to perform simple text **vectorisation**. And also add special tokens such as `<unk>` to replace unknown/out-of-vocabulary (OOV) words, and `<pad>` to pad inputs to a given length.\n",
    "\n",
    "\n",
    "1. First pre-process/normalise the text.\n",
    "\n",
    "2. Then use `Counter()` to create a dictionary of words and frequency counts.\n",
    "\n",
    "3. Finally create a vocabulary (list) and add the `vocab_size` number of most frequently occurring words. Note we also added `<unk>` and `<pad>` at the beginning. We will use these in future weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Let's visit my father-in-law in St. Louis next year.\n",
      " He said 'it would be fun!'.\n",
      "Counter({'-': 2, 'in': 2, '.': 2, \"'\": 2, 'let': 1, \"'s\": 1, 'visit': 1, 'my': 1, 'father': 1, 'law': 1, 'st.': 1, 'louis': 1, 'next': 1, 'year': 1, '': 1, 'he': 1, 'said': 1, 'it': 1, 'would': 1, 'be': 1, 'fun': 1, '!': 1})\n",
      "Total word count: 22\n",
      "['<pad>', '<unk>', '-', 'in', '.', \"'\", 'let', \"'s\", 'visit', 'my', 'father', 'law', 'st.', 'louis', 'next', 'year', '', 'he', 'said', 'it']\n",
      "Vocabulary size: 20\n",
      "Vocab index and token:\n",
      "10\n",
      "father\n"
     ]
    }
   ],
   "source": [
    "# Create spacy document object\n",
    "raw_text = \"Let's visit my father-in-law in St. Louis next year.\\n He said 'it would be fun!'.\"\n",
    "doc = nlp(raw_text)\n",
    "print(f\"Document: {doc}\")\n",
    "\n",
    "# Do some pre-processing\n",
    "# Let's just lowercase the tokens and remove whitespace characters\n",
    "corpus = []\n",
    "for sent in doc.sents:\n",
    "    sent = [token.lower_ for token in sent]\n",
    "    sent = [token.strip() for token in sent]\n",
    "    corpus.append(sent)\n",
    "\n",
    "# Count the frequency of each token in the corpus\n",
    "word_counter = Counter()\n",
    "for sent in corpus:\n",
    "    word_counter.update(sent)\n",
    "print(word_counter)\n",
    "print(f\"Total word count: {len(word_counter)}\")\n",
    "\n",
    "# Create a vocabulary of vocab_size, also include special tokens\n",
    "vocab_size = 20\n",
    "special_tokens = ['<pad>', '<unk>']\n",
    "vocab = []\n",
    "\n",
    "# Add the special tokens to the vocabulary\n",
    "vocab.extend(special_tokens)\n",
    "\n",
    "# Add the vocab_size most common tokens to the vocabulary\n",
    "vocab.extend([word for word, count in word_counter.most_common(vocab_size - len(special_tokens))])\n",
    "print(vocab)\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# Now we can get the index for a token, or the token from an index\n",
    "print(\"Vocab index and token:\")\n",
    "print(vocab.index('father'))\n",
    "print(vocab[vocab.index('father')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "803f0b95e2e5f83dfbc7769a9a1838e6",
     "grade": false,
     "grade_id": "cell-73ef3f5271279f67",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style=\"color:black\"><h2>1.3 Exercise: Pre-processing pipeline</h2>\n",
    "\n",
    "We have now seen each various pre-processing options applied individually. However, several of these steps will need to be applied at the same time. The appropriate steps to apply are problem specific and choice of approach is all part of a NLP project development. At the very least you will probably need to remove extra whitespace and tokenise the text, but most likely case-folding and removing some special characters will be necessary too. \n",
    "\n",
    "Libraries like NLTK, spaCy and textaCy can help you build a processing 'pipeline' but it is sometimes convenient to create a function or class to handle these steps for you.\n",
    "\n",
    "1. In the following cell complete the `preprocess_text()` function. It should take a single string as input, apply a range of processing options and either return a list of tokens, if `tokenise=True`, or a string. Hint: to return a string you may use the `_join_punctuation()` function given.\n",
    "\n",
    "2. The function should be able to apply case-folding, expand contractions, lemmatise, remove punctuation, whitespace, accents and basic HTML tags. It should also include arguments to select and apply each of these options separately e.g. `to_lower=False`.\n",
    "\n",
    "3. You can use the `test_text` string to develop the function. Remember the *order* you apply different steps can make a big difference!\n",
    "\n",
    "<b>MARKS AVAILABLE: 5</b>\n",
    "<br>\n",
    "<b>MO1</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6e6548c9b115906be4db7664adb55e3",
     "grade": false,
     "grade_id": "cell-d38aa677f60b9f9b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text:\n",
      " <a href='https://www.imdb.com/title/tt0000417/reviews/?ref_=tt_ql_urv'>I can now say that I've seen a movie that's over 100 years old</a> Georges M√©li√®s's 1902 masterpiece is not just a science fiction movie. <br /><br />It's also a satire on nineteenth-century science.\tLe Voyage dans la Lune (A Trip to the Moon) is also an indictment of colonialism.\n",
      "The astronauts attack the Moon Men - called \"Selenites\" - and then bring one back to Earth, where they parade him around. \n",
      "\n",
      "Preprocessed text:\n",
      " ['I', 'can', 'now', 'say', 'that', 'I', 'have', 'see', 'a', 'movie', 'that', 'be', 'over', '100', 'year', 'old', 'george', 'melie', 's', '1902', 'masterpiece', 'be', 'not', 'just', 'a', 'science', 'fiction', 'movie', 'it', 'be', 'also', 'a', 'satire', 'on', 'nineteenth', 'century', 'science', 'le', 'voyage', 'dans', 'la', 'lune', 'a', 'trip', 'to', 'the', 'moon', 'be', 'also', 'an', 'indictment', 'of', 'colonialism', 'the', 'astronaut', 'attack', 'the', 'moon', 'man', 'call', 'selenite', 'and', 'then', 'bring', 'one', 'back', 'to', 'earth', 'where', 'they', 'parade', 'he', 'around']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, tokenise=False, to_lower=False, remove_punct=False, remove_space=False, exp_contractions=False, remove_accents=False, remove_html=False, lemmatise=False):\n",
    "    \"\"\"Arguments:\n",
    "        text (str): The text to be preprocessed\n",
    "        tokenise (bool): Whether to return a list of tokens or a string\n",
    "        to_lower (bool): Whether to convert text to lowercase\n",
    "        remove_punct (bool/str): Whether to remove punctuation or a specific character\n",
    "        remove_space (bool): Whether to remove whitespace characters\n",
    "        exp_contractions (bool): Whether to expand contractions\n",
    "        remove_accents (bool): Whether to remove accents\n",
    "        remove_html (bool): Whether to remove HTML tags\n",
    "        lemmatise (bool): Whether to lemmatise tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if 'text' is not of type string\n",
    "    if not isinstance(text, str): \n",
    "        # Return an empty string if an input is not valid\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to lowercase before tokenisation if needed\n",
    "    if to_lower:\n",
    "        text = text.lower()\n",
    "\n",
    "    # Expand contractions\n",
    "    if exp_contractions:\n",
    "        text = contractions.fix(text)\n",
    "\n",
    "    # Remove accents\n",
    "    if remove_accents:\n",
    "        text = unidecode.unidecode(text)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    if remove_html:\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        \n",
    "    #  Remove punctuation before tokenisation using regex\n",
    "    if remove_punct:\n",
    "        text = re.sub('[^A-Za-z0-9\\s]+', ' ', text)  # This removes all punctuation and special characters, keeping only letters, numbers, and spaces.\n",
    "\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    # Regex: remove 1 or more whitespace characters\n",
    "    # re.sub('\\s+', ' ', text)\n",
    "    if remove_space:\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenisation (Using SpaCy)\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ if lemmatise else token.text \n",
    "              for token in doc if not token.is_space]\n",
    "\n",
    "\n",
    "    def _join_punctuation(tokens, characters=\".,;?!\"):\n",
    "        characters = set(characters)\n",
    "        tokens = iter(tokens)\n",
    "        current = next(tokens)\n",
    "\n",
    "        for next_token in tokens:\n",
    "            if any((char in characters) for char in next_token):\n",
    "                current += next_token\n",
    "            else:\n",
    "                yield current\n",
    "                current = next_token\n",
    "\n",
    "        yield current\n",
    "\n",
    "    # Return string or tokens\n",
    "    if tokenise:\n",
    "        return tokens\n",
    "    else:\n",
    "        string =  \" \".join(_join_punctuation(tokens))\n",
    "        return string\n",
    "    \n",
    "# Text for testing\n",
    "\n",
    "test_text = \"<a href='https://www.imdb.com/title/tt0000417/reviews/?ref_=tt_ql_urv'>I can now say that I've seen a movie that's over 100 years old</a> Georges M√©li√®s's 1902 masterpiece is not just a science fiction movie. <br /><br />It's also a satire on nineteenth-century science.\\t\"\"Le Voyage dans la Lune\"\" (\"\"A Trip to the Moon\"\") is also an indictment of colonialism.\\nThe astronauts attack the Moon Men - called \\\"Selenites\\\" - and then bring one back to Earth, where they parade him around. \"\"\"\n",
    "print(f\"Test text:\\n {test_text}\")\n",
    "\n",
    "processed_text = preprocess_text(test_text, tokenise=True, to_lower=True, remove_punct=True, remove_space=True, exp_contractions=True, remove_accents=True, remove_html=True, lemmatise=True)\n",
    "print(f\"\\nPreprocessed text:\\n {processed_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87b8c7c785a4a60e393152f86e11a7ca",
     "grade": true,
     "grade_id": "cell-ef69d9685cd174c7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test cell (1 mark)\n",
    "\n",
    "# Test tokenisation\n",
    "assert preprocess_text(\"Text to #tokenise, but don't just split!\", tokenise=True) == ['Text', 'to', '#', 'tokenise', ',', 'but', 'do', \"n't\", 'just', 'split', '!']\n",
    "\n",
    "# Test lowercase\n",
    "assert preprocess_text(\"LOWERCASE this Text.\", to_lower=True) == \"lowercase this text.\"\n",
    "\n",
    "# Test remove punctuation\n",
    "assert preprocess_text(\"Remove the punctuation, from this #text.\", remove_punct=True) == \"Remove the punctuation from this text\"\n",
    "\n",
    "# Test remove whitespace\n",
    "assert preprocess_text(\"Remove     the extra\\t, whitespace \\n from this text.    \", remove_space=True) == \"Remove the extra, whitespace from this text.\"\n",
    "\n",
    "# Test expand contractions\n",
    "assert preprocess_text(\"Don't forget you'll need to expand contractions.\", exp_contractions=True) == \"Do not forget you will need to expand contractions.\"\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "79f583ddd23acbee056c696cb0abcfb1",
     "grade": true,
     "grade_id": "cell-889f9cdcc65d8b56",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test cell (2 marks)\n",
    "\n",
    "# Test remove accents\n",
    "assert preprocess_text(\"The Jalape√±o Caf√© has a strange fa√ßade.\", remove_accents=True) == \"The Jalapeno Cafe has a strange facade.\"\n",
    "\n",
    "# Test remove HTML tags\n",
    "assert preprocess_text(\"<a href='site.com'>Get the text from the link, <b>remove HTML tags!</b></a>\", remove_html=True) == \"Get the text from the link, remove HTML tags!\"\n",
    "\n",
    "# Test lemmatisation\n",
    "assert preprocess_text(\"I am studying lemmatisation, it is useful.\", lemmatise=True) == \"I be study lemmatisation, it be useful.\"\n",
    "\n",
    "# Test lowercase, remove punctuation but don't tokenise\n",
    "assert preprocess_text(\"Tokenise, remove punc, order-matters and LOWERCASE\", tokenise=False, remove_punct=True, to_lower=True) == \"tokenise remove punc order matters and lowercase\"\n",
    "\n",
    "# Test all options\n",
    "assert preprocess_text(\"<b>Tokenise, remove punc, order-matters LOWERCASE\\t and and lemmatise it's useful at the Caf√©.</b>\\n\", tokenise=True, remove_punct=True, to_lower=True, remove_space=True, exp_contractions=True, remove_accents=True, remove_html=True, lemmatise=True) == ['tokenise', 'remove', 'punc', 'order', 'matter', 'lowercase', 'and', 'and', 'lemmatise', 'it', 'be', 'useful', 'at', 'the', 'cafe']\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "537ac471aae0f6958d0543cbedc2a383",
     "grade": true,
     "grade_id": "cell-d147ef35ed2d52f5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden test cell (2 marks)\n",
    "# Tests a few different combinations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d66a1be4584a5c6890dd0530882d85c",
     "grade": false,
     "grade_id": "cell-5bdd971419a43505",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style=\"color:black\"><h2>1.4 Exercise: Pre-processing IMDB</h2>\n",
    "\n",
    "Now that we have a pre-processing function, let's use it to process the IMDB reviews that you annotated! Once you are happy with the function, in the next cell:\n",
    "\n",
    "1. First load your IMDB reviews from the `imdb_reviews_annot.csv` file. \n",
    "\n",
    "2. Create a new dataframe called `imdb_corpus` with two columns `review` and `sentiment` for the processed reviews and the sentiment labels.\n",
    "\n",
    "3. Now apply the `preprocess_text()` function to each review. You should **NOT** tokenise at this stage (we will do this later), but at minimum you should remove extra whitespace, lowercase, remove punctuation, remove HTML and expand contractions. Otherwise you are welcome to use other pre-processing options for your reviews.\n",
    "\n",
    "<b>MARKS AVAILABLE: 5</b>\n",
    "<br>\n",
    "<b>MO1</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6187b198d9941b8d2245f0511fcdcae0",
     "grade": false,
     "grade_id": "cell-ea1a36adfb72e4e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews: 56\n",
      "Number of negative reviews: 44\n",
      "Number of unlabelled reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "reviews_df = pd.read_csv(os.path.join(data_dir, 'imdb_reviews_annot.csv'), index_col=0)\n",
    "\n",
    "# Check the labelled reviews to check if there are unlabelled reviews\n",
    "print(f\"Number of positive reviews: {(reviews_df['sentiment'] == 'positive').sum()}\")\n",
    "print(f\"Number of negative reviews: {(reviews_df['sentiment'] == 'negative').sum()}\")\n",
    "print(f\"Number of unlabelled reviews: {reviews_df['sentiment'].isnull().sum()}\")\n",
    "\n",
    "\n",
    "# Create new dataframe for processed reviews\n",
    "imdb_corpus = pd.DataFrame()\n",
    "\n",
    "def process_review(text):\n",
    "    processed_text = preprocess_text(\n",
    "        text=text,\n",
    "        tokenise=False,        \n",
    "        to_lower=True,          \n",
    "        remove_punct=True,      \n",
    "        remove_space=True,      \n",
    "        exp_contractions=True,\n",
    "        remove_accents=True,  \n",
    "        remove_html=True,       \n",
    "        lemmatise=False  \n",
    "    )\n",
    "\n",
    "    processed_text = processed_text.strip()  \n",
    "    processed_text = \" \".join(processed_text.split()) \n",
    "    return processed_text\n",
    "\n",
    "imdb_corpus['review'] = reviews_df['review'].astype(str).apply(process_review)\n",
    "imdb_corpus['sentiment'] = reviews_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec91bf4ef55cbb52fc1ff1a3e6a16970",
     "grade": true,
     "grade_id": "cell-cf169624ca0eba5d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test cell (1 mark)\n",
    "\n",
    "# Test the dataframe has correct number of rows and columns\n",
    "assert imdb_corpus.shape == (100, 2)\n",
    "assert list(imdb_corpus.columns) == ['review', 'sentiment']\n",
    "\n",
    "# Test sentiment is only 'positive' or 'negative'\n",
    "assert set(imdb_corpus['sentiment'].unique()) == {'positive', 'negative'}\n",
    "\n",
    "# Test reviews are NOT tokenised\n",
    "assert pd.api.types.infer_dtype(imdb_corpus['review']) == 'string'\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16bdb56cd5a2dbc16e87cdbfa1769ba9",
     "grade": true,
     "grade_id": "cell-ebd90108aa80ced5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test cell (2 marks)\n",
    "\n",
    "# Test reviews are lowercased\n",
    "assert imdb_corpus['review'].str.islower().all()\n",
    "\n",
    "# Test reviews have no punctuation\n",
    "assert imdb_corpus['review'].str.match(r'^[A-Za-z0-9\\s]+$').all()\n",
    "\n",
    "# Test reviews have no whitespace characters\n",
    "assert not imdb_corpus['review'].str.match(r'\\s+').any()\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73fa15ec9e1838196d354b1d4c01e955",
     "grade": true,
     "grade_id": "cell-02c16b87f595264f",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden tests (2 marks)\n",
    "# Test remaining preprocessing steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0677be66a59a48d83285137e8987810",
     "grade": false,
     "grade_id": "cell-dc8922cc62ec32f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Make sure to save the dataframe to a new file called `imdb_reviews.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe\n",
    "imdb_corpus.to_csv(os.path.join(data_dir, 'imdb_reviews.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "262faa62398b1c442177c3bdd000b477",
     "grade": false,
     "grade_id": "cell-2498f73dae1a121c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color:black\"><h3>Before you submit this notebook to NBGrader for marking:</h3> \n",
    "\n",
    "1. Make sure have completed all exercises marked by <span style=\"color:blue\">**blue cells**</span>.\n",
    "2. For automatically marked exercises ensure you have completed any cells with `# YOUR CODE HERE`. Then click 'Validate' button above, or ensure all cells run without producing an error.\n",
    "3. For manually marked exercises ensure you have completed any cells with `\"YOUR ANSWER HERE\"`.\n",
    "4. Ensure all cells are run with their output visible.\n",
    "5. Fill in your student ID (**only**) below.\n",
    "6. You should now **save and download** your work.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID:** 15006280"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_ai",
   "language": "python",
   "name": "adv_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
