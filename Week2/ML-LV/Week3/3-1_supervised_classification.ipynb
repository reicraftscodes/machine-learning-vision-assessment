{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4312a368c81274c9b2566bd788c87e9e",
     "grade": false,
     "grade_id": "cell-1c031e70e9fbf508",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-danger\" style=\"color:black\"><b>Running ML-LV Jupyter Notebooks:</b><br>\n",
    "    <ol>\n",
    "        <li>Make sure you are running all notebooks using the <code>adv_ai</code> kernel.\n",
    "        <li><b>It is very important that you do not create any additional files within the weekly folders on CSCT cloud.</b> Any additional files, or editing the notebooks with a different environment may prevent submission/marking of your work.</li>\n",
    "            <ul>\n",
    "                <li>NBGrader will automatically fetch and create the correct folders files for you.</li>\n",
    "                <li>All files that are not the Jupyter notebooks should be stored in the 'ML-LV/data' directory.</li>\n",
    "            </ul>\n",
    "        <li>Please <b>do not pip install</b> any python packages (or anything else). You should not need to install anything to complete these notebooks other than the packages provided in the Jupyter CSCT Cloud environment.</li>\n",
    "    </ol>\n",
    "    <b>If you would like to run this notebook locally you should:</b><br>\n",
    "    <ol>\n",
    "        <li>Create an environment using the requirements.txt file provided. <b>Any additional packages you install will not be accessible when uploaded to the server and may prevent marking.</b></li>\n",
    "        <li>Download a copy  of the notebook to your own machine. You can then edit the cells as you wish and then go back and copy the code into/edit the ones on the CSCT cloud in-place.</li>\n",
    "        <li><b>It is very important that you do not re-upload any notebooks that you have edited locally.</b> This is because NBGrader uses cell metadata to track marked tasks. <b>If you change this format it may prevent marking.</b></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ce55f007d6970443834dcca6f1c577f",
     "grade": false,
     "grade_id": "cell-2bd7e391172324be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Practical 3: Text Classification\n",
    "\n",
    "In the previous practicals we created some IMDB movie review data for sentiment analysis and explored several text pre-processing and representation methods. By now you should have pre-processed the reviews you scraped from IMDB and also the full 50,000 review dataset. So, now we are ready to train a model to classify the sentiment of our movie reviews! We will explore several unsupervised and supervised approaches, using an existing movie review dataset for training and keep ours as an additional test set.\n",
    "\n",
    "In the first part of this practical we will explore two supervised classification algorithms, Naive Bayes and an Artificial Neural Network (ANN).\n",
    "\n",
    "In the second part of this practical we will look at several unsupervised algorithms K-means clustering and Semantic Analysis using word embeddings.\n",
    "\n",
    "The objectives of this practical are:\n",
    "\n",
    "1. Apply a complete NLP workflow for text classification\n",
    "\n",
    "2. Understand the probabilistic Naive Bayes classifier and consider different aspects of applying an ANN to text data\n",
    "\n",
    "3. Consider appropriate representations for unsupervised text classification, including clustering and semantic analysis with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72eafce2a1c31243c3ad5f89462591ff",
     "grade": false,
     "grade_id": "cell-bf7b3f7543679419",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1 Supervised Text Classification\n",
    "\n",
    "## 1.0 Import libraries\n",
    "\n",
    "1. [Tensorflow](https://www.tensorflow.org/) - is a powerful Python library for machine learning.\n",
    "\n",
    "2. [Keras](https://keras.io/) - is a simple API for building machine learning models and is built into Tensorflow 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress Tensorflow messages\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the status of NBgrader (for skipping cell execution while validating/grading)\n",
    "grading = True if os.getenv('NBGRADER_EXECUTION') else False\n",
    "\n",
    "# Set seaborn style for matplotlib plots\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "\n",
    "# Get the project directory (should be in ML-LV)\n",
    "path = ''\n",
    "while os.path.basename(os.path.abspath(path)) != 'ML-LV':\n",
    "    path = os.path.abspath(os.path.join(path, '..'))\n",
    "\n",
    "# Set the directory to the data folder (should be in ML-LV/data/imdb)\n",
    "data_dir = os.path.join(path, 'data', 'imdb')\n",
    "\n",
    "# Set the directory to the shared dataset folder (should be in shared/datasets/imdb)\n",
    "dataset_dir = os.path.join(path, '..', 'shared', 'datasets', 'imdb')\n",
    "\n",
    "# Load the Spacy language model ('en_core_web_md' should be in shared/models/spacy)\n",
    "nlp = spacy.load(os.path.join(path, '..', 'shared', 'models', 'spacy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ff8abab800ce5f3ac8b4cf100ed11f4",
     "grade": false,
     "grade_id": "cell-8ae10fff3002b8d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Load and pre-process data\n",
    "\n",
    "1. First load the full IMDB dataset and our smaller reviews set.\n",
    "\n",
    "2. Then we need to convert the 'positive' and 'negative' class labels to numerical values, 1 for positive and 0 for negative. Using the pandas `get_dummies` function creates two binary valued columns and then the `drop_first` parameter collapses these into a single column.\n",
    "\n",
    "3. The next cell plots the distribution of review sentiment for the dataset and our reviews. As you can, see the classes are perfectly balanced within the dataset, but are they in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb dataset\n",
    "imdb_dataset = pd.read_csv(os.path.join(dataset_dir, 'imdb_dataset.csv'))\n",
    "\n",
    "# Load your imdb reviews\n",
    "imdb_reviews = pd.read_csv(os.path.join(data_dir, 'imdb_reviews.csv'))\n",
    "\n",
    "# # Convert the sentiment to a binary value\n",
    "imdb_dataset['sentiment'] = pd.get_dummies(imdb_dataset['sentiment'], drop_first=True)\n",
    "imdb_reviews['sentiment'] = pd.get_dummies(imdb_reviews['sentiment'], drop_first=True)\n",
    "\n",
    "imdb_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 6))\n",
    "ax[0].set_title('IMDB Dataset')\n",
    "ax[0].bar(['Positive', 'Negative'], height=imdb_dataset['sentiment'].value_counts(), color=['tab:red', 'tab:blue'])\n",
    "ax[1].set_title('IMDB Reviews')\n",
    "ax[1].bar(['Positive', 'Negative'], height=imdb_reviews['sentiment'].value_counts(), color=['tab:red', 'tab:blue'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85282c4f17afd1707ce21100a3fac8c6",
     "grade": false,
     "grade_id": "cell-497c2c5125e63c35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Process and vectorise the text\n",
    "\n",
    "1. We will use sklearn's `CountVectorizer()` to tokenise the text and vectorise each review into a BOW. We will also remove english stop words\n",
    "\n",
    "2. Once the text is vectorised, split into training and validation sets. Use your IMDB reviews as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the vocab size\n",
    "vocab_size = 5000\n",
    "\n",
    "# Create a CountVectorizer\n",
    "bow_vectoriser = CountVectorizer(max_features=vocab_size, stop_words='english')\n",
    "\n",
    "# Vectorise the text\n",
    "X = bow_vectoriser.fit_transform(imdb_dataset['review']).toarray()\n",
    "print(f'Shape of X: {X.shape}')\n",
    "print(X[:5, :])\n",
    "\n",
    "# Get the class labels\n",
    "y = imdb_dataset['sentiment'].values\n",
    "print(f'Shape of y: {y.shape}')\n",
    "print(y[:5])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorise the text for the test set\n",
    "X_test = bow_vectoriser.transform(imdb_reviews['review']).toarray()\n",
    "\n",
    "# Get the class labels\n",
    "y_test = imdb_reviews['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b6f0a8d91b5deba94b58138583782fd",
     "grade": false,
     "grade_id": "cell-2a3bfd1ee89db59e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Naive Bayes\n",
    "\n",
    "Naïve Bayes is a generative classification algorithm which finds the probability of an event based on prior knowledge/examples of similar events. It is naïve, because it assumes that each feature is independent (do not effect each other) so we can calculate probabilities independently.\n",
    "\n",
    "The class defined below implements the following formulation of the algorithm:\n",
    "\n",
    "$ \\hat{y} = argmax(log(P(y) + \\sum_{i=1}^{n} P(x_i|y))) $\n",
    "\n",
    "Where:\n",
    "\n",
    "$ \\hat{y} = $ the predicted label\n",
    "\n",
    "$ P(y) = $ the probability of class y\n",
    "\n",
    "$ P(x_i|y) = $ the product of the probability that feature $i$ in $x$ occurs, given $y$\n",
    "\n",
    "By default the algorithm uses:\n",
    "- a laplace smoothing parameter `alpha=1.0`, which prevents division by zero when calculating likelihoods for words that do not appear in the training data for a given class.\n",
    "- and `use_log=True`, to calculates probabilities in log space which numerically more stable.\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color:black\"><b>Note:</b> For a thorough discussion of Naïve Bayes, including smoothing, logs and several Python implementations see <a href='https://sidsite.com/posts/implementing-naive-bayes-in-python/'> this </a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    \"\"\"Naive Bayes classifier for categorical data.\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, use_log=True):\n",
    "        \"\"\" Arguments:\n",
    "                alpha: Laplace smoothing parameter.\n",
    "                use_log: Use log probabilities to avoid underflow.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha # Smoothing parameter. Prevents division by zero when calculating likelihoods.\n",
    "        self.use_log = use_log # Use log probabilities to avoid underflow.\n",
    "        self.prior = None # The prior (mu) distribution of class labels. The probability of each class, P(class) within the training data.\n",
    "        self.multinomial = None # The multinomial distribution (phi) is the probability/likelihood of each feature conditioned on the class, P(feature | class).\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data for Naive Bayes classifier.\"\"\"\n",
    "\n",
    "        # N is the number of examples\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Calculate prior\n",
    "        # Split the input array into sub-arrays depending on class label\n",
    "        X_by_class = np.array([X[y == class_lbl] for class_lbl in np.unique(y)], dtype=object)\n",
    "        \n",
    "        # Count the number of examples in each class and divide by total number of examples\n",
    "        self.prior = np.array([X_class.shape[0] / N for X_class in X_by_class])\n",
    "        assert len(self.prior) == len(np.unique(y)), 'Number of priors should equal number of classes'\n",
    "\n",
    "        # Calculate multinomial coefficients\n",
    "        # Create array of shape (num_classes, num_features) to hold multinomial coefficients\n",
    "        self.multinomial = np.zeros((len(np.unique(y)), X.shape[1]))\n",
    "\n",
    "        for class_lbl in range(len(self.prior)):\n",
    "\n",
    "            # Count the number of times each feature appears in all examples of a particular class + alpha\n",
    "            class_feature_counts = X_by_class[class_lbl].sum(axis=0) + self.alpha\n",
    "\n",
    "            # Probability of each feature given the class\n",
    "            # Individual feature counts divided by the total number of times all features appear in the class\n",
    "            self.multinomial[class_lbl] = class_feature_counts / class_feature_counts.sum()\n",
    "        \n",
    "        # Convert to log probabilities\n",
    "        if self.use_log:\n",
    "            self.prior = np.log(self.prior)\n",
    "            self.multinomial = np.log(self.multinomial)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probability of class for each input example.\"\"\"\n",
    "\n",
    "        # Create array of shape (num_examples, num_classes) to store class probabilities (posterior) for each example\n",
    "        class_probabilities = np.zeros(shape=(X.shape[0], self.prior.shape[0]))\n",
    "\n",
    "        # Loop over each example and calculate individual conditional likelihoods for each class,\n",
    "        # then multiply them all together (the product), and multiply by the class priors,\n",
    "        # or in log space, add them all together (the sum), and add the class priors.\n",
    "        for i, example in enumerate(X):\n",
    "            example_likelihood = []\n",
    "\n",
    "            # Loop over each class\n",
    "            for class_lbl in range(len(self.prior)):\n",
    "                feature_likelihood = []\n",
    "\n",
    "                # Loop over each feature\n",
    "                for feature in range(example.shape[0]):\n",
    "                    # If the feature is present in the example\n",
    "                    if example[feature] > 0:\n",
    "                        # Calculate the probability of the feature given the class (multinomial coefficient */+ feature count)\n",
    "                        mn_coefficient = self.multinomial[class_lbl][feature]\n",
    "                        # If using log space the convert the example feature count to log space\n",
    "                        if self.use_log:\n",
    "                            feature_likelihood.append(mn_coefficient + np.log(example[feature]))\n",
    "                        else:\n",
    "                            feature_likelihood.append(mn_coefficient ** example[feature])\n",
    "\n",
    "                # Append the probabilties of this class for this example\n",
    "                example_likelihood.append(feature_likelihood)\n",
    "\n",
    "            # Calculate joint probabilities\n",
    "            # Multiply (or sum) all the individual feature probabilities together and multiply by (or add) class priors\n",
    "            if self.use_log:\n",
    "                class_probabilities[i] = np.asarray(example_likelihood).sum(axis=1) + self.prior\n",
    "            else:\n",
    "                class_probabilities[i] = np.asarray(example_likelihood).prod(axis=1) * self.prior\n",
    "        \n",
    "        # Normalise so probabilities sum to 1\n",
    "        class_probabilities = class_probabilities / np.linalg.norm(class_probabilities, ord=1, axis=1, keepdims=True)\n",
    "        assert (class_probabilities.sum(axis=1) - 1 < 0.001).all(), 'Rows should sum to 1'\n",
    "\n",
    "        return class_probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class with highest probability.\"\"\"\n",
    "        return self.predict_proba(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e109c84eda9314586b4bad4b7bbd4de0",
     "grade": false,
     "grade_id": "cell-3775d285ab38f428",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grading:\n",
    "    # Create and train a Naive Bayes classifier\n",
    "    nb_model = NaiveBayes()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict class labels for validation set\n",
    "    predictions = nb_model.predict(X_val)\n",
    "    print(f'Validation Accuracy: {accuracy_score(y_val, predictions)}')\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    conf_matrix = ConfusionMatrixDisplay.from_predictions(y_val, predictions, display_labels=['negative', 'positive'], colorbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e9664a2f500066d4ca091df35d28ba0",
     "grade": false,
     "grade_id": "cell-8061f2fe821187fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluate the model on your IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grading:\n",
    "    # Predict class labels for test set\n",
    "    predictions = nb_model.predict(X_test)\n",
    "    print(f'Test Accuracy: {accuracy_score(y_test, predictions)}')\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    conf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, predictions, display_labels=['negative', 'positive'], colorbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7717376157ec34c76c3b215f5105568",
     "grade": false,
     "grade_id": "cell-8bfa9a3951db6e52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style=\"color:black\"><h2>1.3 Exercise: Artificial Neural Network</h2>\n",
    "\n",
    "ANN are a discriminative classification algorithm which learn a decision boundary to separate the classes and select appropriate labels based on the input features.\n",
    "\n",
    "1. In the following cell complete the `build_model()` function. It should take in the following arguments and return a Keras sequential model:\n",
    "    - `vocab_size` is the input shape, because each word is a feature.\n",
    "    - `layers_list` is a list of tuples containing the number of neurons and activation function for each layer. Thus the length of the list is also the number of layers (input + hidden). For example: `[(10, 'sigmoid'), (5, 'sigmoid')]` is two layers with sigmoid activation and 10 and 5 units respectively. **Note:** this *does not* include the final output layer.\n",
    "    - `dropout` is the dropout rate for each intermediate dropout layer after a hidden layer.\n",
    "    - `n_class` is the number of classes to predict, or the shape of the output layer.\n",
    "    - `output_activation` is the activation function for the output layer.\n",
    "    - `optimiser` is the function for calculating the gradient and updating the weights. Should be any valid Keras optimiser e.g. `SGD`.\n",
    "    - `loss` is the loss function for calculating the error of the model. Should be any valid Keras loss e.g. `categorical_crossentropy`.\n",
    "    - `name` is just a name to identify the model.\n",
    "\n",
    "2. The first layer should be an input layer, followed by one hidden layer and one dropout layer for each tuple in `layers_list`. The last layer should be the classification/output layer for `n_class` with `output_activation`.\n",
    "\n",
    "3. Once all the layers are implemented the function should compile the model with the given optimiser, loss and the accuracy metric (`metrics=[\"accuracy\"]`).\n",
    "\n",
    "4. Finally, return the compiled model.\n",
    "\n",
    "<b>MARKS AVAILABLE: 5</b>\n",
    "<br>\n",
    "<b>MO1</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b02d139f2724695c9006999fdb74964",
     "grade": false,
     "grade_id": "cell-3b8b241fe3f768da",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, layers_list, dropout, n_class, output_activation, optimiser, loss, name):\n",
    "    \"\"\"Build a simple ANN model\n",
    "\n",
    "    Arguments:\n",
    "        vocab_size (int): Size of the vocabulary\n",
    "        layers_list (list): A list of tuples containing the number of neurons (int) and activation function (str) for each layer\n",
    "        dropout (int): The dropout rate\n",
    "        n_class (int): Number of classes\n",
    "        output_activation (str): Activation function for the output layer\n",
    "        optimiser (str): Optimiser to use\n",
    "        loss (str): Loss function to use\n",
    "        name (str): Name of the model\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68951d4683a2215b9458fbc542cf9d8e",
     "grade": true,
     "grade_id": "cell-8781320f7a55d6ea",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell (3 marks)\n",
    "\n",
    "# Create a model using the build_model function\n",
    "model = build_model(200, [(10, 'sigmoid')], 0.1, 5, 'softmax', 'sgd', 'categorical_crossentropy', 'a_model')\n",
    "\n",
    "# Test the size of the input shape\n",
    "assert model.input_shape == (None, 200), 'Input shape should be (None, 200)'\n",
    "\n",
    "# Test the number of layers\n",
    "assert len(model.layers) == 3, 'Model should have 3 layers'\n",
    "\n",
    "# Test the number of neurons in the first layer\n",
    "assert model.layers[0].units == 10, 'First layer should have 10 neurons'\n",
    "\n",
    "# Test the activation function of the first layer\n",
    "assert model.layers[0].activation.__name__ == 'sigmoid', 'First layer should have sigmoid activation'\n",
    "\n",
    "# Test there is a dropout layer after the first layer\n",
    "assert isinstance(model.layers[1], layers.Dropout), 'Second layer should be a dropout layer'\n",
    "\n",
    "# Test the dropout rate\n",
    "assert model.layers[1].rate == 0.1, 'Dropout rate should be 0.1'\n",
    "\n",
    "# Test the number of neurons in the second layer\n",
    "assert model.layers[2].units == 5, 'Second layer should have 5 neurons'\n",
    "\n",
    "# Test the activation function of the second layer\n",
    "assert model.layers[2].activation.__name__ == 'softmax', 'Second layer should have softmax activation'\n",
    "\n",
    "# Test the loss function\n",
    "assert model.loss == 'categorical_crossentropy', 'Loss should be categorical_crossentropy'\n",
    "\n",
    "# Test the learning rate\n",
    "assert model.optimizer.learning_rate == 0.01, 'Learning rate should be 0.01'\n",
    "\n",
    "# Test the optimiser\n",
    "assert model.optimizer.name == 'SGD', 'Optimizer should be SGD'\n",
    "\n",
    "# Test the name\n",
    "assert model.name == 'a_model', 'Model name should be a_model'\n",
    "\n",
    "# Test the model type\n",
    "assert isinstance(model, models.Sequential), 'Model should be a Sequential model'\n",
    "\n",
    "# Test the number of parameters\n",
    "assert model.count_params() == 2065, 'Model should have 2065 parameters'\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba34c5f98645c365c9096e271243c1af",
     "grade": true,
     "grade_id": "cell-5e900be6b8eb53c5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden test cell (2 marks)\n",
    "# Tests all function parameters with different values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6508da09eb356b23142a3e454a3090b",
     "grade": false,
     "grade_id": "cell-929fb6e3321ca172",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style=\"color:black\"><h2>1.4 Exercise: Build a model for classifying the IMDB reviews</h2>\n",
    "\n",
    "Once you have the `build_model()` function working correctly, select some appropriate parameters and build a model for classifying the IMDB reviews. Adhere to the following constraints:\n",
    "\n",
    "1. Ensure you use the `vocab_size` defined above (5000).\n",
    "\n",
    "2. Do not create more than 4 hidden layers.\n",
    "\n",
    "3. The total number of parameters should be < 300,000.\n",
    "\n",
    "<b>MARKS AVAILABLE: 5</b>\n",
    "<br>\n",
    "<b>MO1</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce8e5a51a94788da8c7fddcc31b3e8ab",
     "grade": false,
     "grade_id": "cell-9ea811d00ccf3c2b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ca59a2d9b73d48a3e2fbeb77b402122",
     "grade": true,
     "grade_id": "cell-787779704d071d4e",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell (3 marks)\n",
    "\n",
    "# Test the size of the input shape\n",
    "assert model.input_shape == (None, 5000), 'Input shape should be (None, 5000)'\n",
    "\n",
    "# Test the number of layers\n",
    "assert len(model.layers) <= 9, 'Model should not have more than 9 layers (4 hidden + 4 dropout and 1 output)'\n",
    "\n",
    "# Test the number of parameters\n",
    "assert model.count_params() <= 300000, 'Model should not have more than 300,000 parameters'\n",
    "\n",
    "print('All tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4321ed812ff3a54ce9ee1a3134f59f4",
     "grade": true,
     "grade_id": "cell-a38c32611ba19d8e",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Hidden test cell (2 marks)\n",
    "# Tests output layer and loss function are appropriate for this problem\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8537a882748d91743370abbb19a59711",
     "grade": false,
     "grade_id": "cell-188f8d5a84d4ed7a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "Now train the model for a few epochs and evaluate on the test set. You should see an improvement over the Naive Bayes model. You can compare your models performance to the 'state-of-the-art' listed [here](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grading:\n",
    "    # Fit the model\n",
    "    results = model.fit(X_train, y_train, epochs=3, batch_size=128, validation_data=(X_val, y_val))\n",
    "    \n",
    "    # Predict class labels for validation set\n",
    "    predictions = model.predict(X_val)\n",
    "    predictions = [0 if x < 0.5 else 1 for x in predictions]  # Convert probabilities to binary\n",
    "    print(f'Validation Accuracy: {accuracy_score(y_val, predictions)}')\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    conf_matrix = ConfusionMatrixDisplay.from_predictions(y_val, predictions, display_labels=['negative', 'positive'], colorbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab11a2b89a40be619bba29403bfbf745",
     "grade": false,
     "grade_id": "cell-e87562010b5b645e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Evaluate the model on your IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not grading:\n",
    "    # Predict class labels for test set\n",
    "    predictions = model.predict(X_test)\n",
    "    predictions = [0 if x < 0.5 else 1 for x in predictions]  # Convert probabilities to binary\n",
    "    print(f'Test Accuracy: {accuracy_score(y_test, predictions)}')\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    conf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, predictions, display_labels=['negative', 'positive'], colorbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a0002a2dd9eadf95f897ccebd50eee9",
     "grade": false,
     "grade_id": "cell-4ee62400ce5aefe9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\" style=\"color:black\"><h2>1.5 Exercise: Naive Bayes and ANN generalisation</h2>\n",
    "\n",
    "1. You should have noticed that the Naive Bayes algorithm and ANN performed quite similarly on the full IMDB dataset. However, the ANN should have generalised to your small IMDB reviews data better than Naive Bayes did (assuming you chose appropriate parameters). Why might this be?\n",
    "\n",
    "2. In general the Naive Bayes algorithm and ANN probably performed much worse on the small IMDB reviews that you gathered and annotated. Why might this be and what could be done to resolve this issue?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8de562d369a736c7f744f12e00ffc183",
     "grade": false,
     "grade_id": "cell-da377af080082f02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color:black\"><h3>Before you submit this notebook to NBGrader for marking:</h3> \n",
    "\n",
    "1. Make sure have completed all exercises marked by <span style=\"color:blue\">**blue cells**</span>.\n",
    "2. For automatically marked exercises ensure you have completed any cells with `# YOUR CODE HERE`. Then click 'Validate' button above, or ensure all cells run without producing an error.\n",
    "3. For manually marked exercises ensure you have completed any cells with `\"YOUR ANSWER HERE\"`.\n",
    "4. Ensure all cells are run with their output visible.\n",
    "5. Fill in your student ID (**only**) below.\n",
    "6. You should now **save and download** your work.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
