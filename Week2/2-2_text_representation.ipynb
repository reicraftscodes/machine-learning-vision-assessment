{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cbc2d6b28de2cc9d206b3add44893203",
     "grade": false,
     "grade_id": "cell-3b0f8c232ee10b15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-danger\" style=\"color:black\"><b>Running ML-LV Jupyter Notebooks:</b><br>\n",
    "    <ol>\n",
    "        <li>Make sure you are running all notebooks using the <code>adv_ai</code> kernel.\n",
    "        <li><b>It is very important that you do not create any additional files within the weekly folders on CSCT cloud.</b> Any additional files, or editing the notebooks with a different environment may prevent submission/marking of your work.</li>\n",
    "            <ul>\n",
    "                <li>NBGrader will automatically fetch and create the correct folders files for you.</li>\n",
    "                <li>All files that are not the Jupyter notebooks should be stored in the 'ML-LV/data' directory.</li>\n",
    "            </ul>\n",
    "        <li>Please <b>do not pip install</b> any python packages (or anything else). You should not need to install anything to complete these notebooks other than the packages provided in the Jupyter CSCT Cloud environment.</li>\n",
    "    </ol>\n",
    "    <b>If you would like to run this notebook locally you should:</b><br>\n",
    "    <ol>\n",
    "        <li>Create an environment using the requirements.txt file provided. <b>Any additional packages you install will not be accessible when uploaded to the server and may prevent marking.</b></li>\n",
    "        <li>Download a copyÂ  of the notebook to your own machine. You can then edit the cells as you wish and then go back and copy the code into/edit the ones on the CSCT cloud in-place.</li>\n",
    "        <li><b>It is very important that you do not re-upload any notebooks that you have edited locally.</b> This is because NBGrader uses cell metadata to track marked tasks. <b>If you change this format it may prevent marking.</b></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54ae6cfaca8cdf6c10af076e06f2efc2",
     "grade": false,
     "grade_id": "cell-377931a44c531503",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2 Language Representation\n",
    "\n",
    "## 2.0 Import libraries\n",
    "\n",
    "1. [Sklearn (scikit-learn)](https://scikit-learn.org/stable/) - is a comprehensive Python library for Machine Learning. We will use its text pre-processing features and also for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 16:44:04.221099: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-26 16:44:04.224108: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-26 16:44:04.233063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-26 16:44:04.250018: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-26 16:44:04.250043: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-26 16:44:04.261446: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-26 16:44:04.952853: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the status of NBgrader (for skipping cell execution while validating/grading)\n",
    "grading = True if os.getenv('NBGRADER_EXECUTION') else False\n",
    "\n",
    "# Increase pandas display width\n",
    "pd.set_option('display.width', 500)\n",
    "# Set seaborn style for matplotlib plots\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "\n",
    "# Get the project directory (should be in ML-LV)\n",
    "path = ''\n",
    "while os.path.basename(os.path.abspath(path)) != 'ML-LV':\n",
    "    path = os.path.abspath(os.path.join(path, '..'))\n",
    "\n",
    "# Set the directory to the data folder (should be in ML-LV/data/imdb)\n",
    "data_dir = os.path.join(path, 'data', 'imdb')\n",
    "\n",
    "# Load the Spacy language model ('en_core_web_md' should be in shared/models/spacy)\n",
    "nlp = spacy.load(os.path.join(path, '..', 'shared', 'models', 'spacy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1747822df82f7c5989bd08007b78acb6",
     "grade": false,
     "grade_id": "cell-72b46ebcc2b2b19c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.1 Representation options\n",
    "\n",
    "The following cells demonstrate each of the language representation options discussed in the lecture. For most we use numpy/plain Python to demonstrate the process and then sklearn's built in functions.\n",
    "\n",
    "Like pre-processing the appropriate representation is dependant on the task, and generally the input *shape* of the data for a given model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3028e94c4d37d86d3aa649b5aa5d4f56",
     "grade": false,
     "grade_id": "cell-c5d62a4c68197a9e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "One-hot encoding converts a word into an array of length `vocab_size`, with a **1** at the index of the words position in the vocabulary and **0's** in every other position. Encoding a sentence then becomes a 2D array of shape `vocab_size` x `sequence_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: This is a test sentence which is a very long test sentence.\n",
      "\n",
      "Tokens: ['This', 'is', 'a', 'test', 'sentence', 'which', 'is', 'a', 'very', 'long', 'test', 'sentence', '.']\n",
      "\n",
      "Vocabulary: ['which', 'test', 'a', '.', 'very', 'is', 'This', 'long', 'sentence']\n",
      "\n",
      "Token indices: [6, 5, 2, 1, 8, 0, 5, 2, 4, 7, 1, 8, 3]\n",
      "\n",
      "One-hot vector with numpy:\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "One-hot vector with sklearn:\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spacy document object\n",
    "text = \"This is a test sentence which is a very long test sentence.\"\n",
    "doc = nlp(text)\n",
    "print(f\"Document: {doc}\\n\")\n",
    "\n",
    "# Tokenise the document\n",
    "tokens = [token.text for token in doc]\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Create simple vocabulary\n",
    "vocab = list(set(tokens))\n",
    "print(f\"Vocabulary: {vocab}\\n\")\n",
    "\n",
    "# Get a list of token indices within vocabulary\n",
    "token_indices = [vocab.index(token) for token in tokens]\n",
    "print(f\"Token indices: {token_indices}\\n\")\n",
    "\n",
    "# Create a one-hot vector with numpy\n",
    "num_unique = len(vocab) # Need to know how many features there are\n",
    "one_hot_np = np.eye(num_unique)[token_indices]\n",
    "print(f\"One-hot vector with numpy:\\n {one_hot_np}\\n\")\n",
    "\n",
    "# Create a one-hot vector with sklearn\n",
    "token_indices = np.array(token_indices).reshape(-1, 1) # Need to reshape the array to 2D\n",
    "one_hot_sk = OneHotEncoder(sparse_output=False).fit_transform(token_indices)\n",
    "print(f\"One-hot vector with sklearn:\\n {one_hot_sk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6e92d4dcb0646ff46ec4354b4e9c84e0",
     "grade": false,
     "grade_id": "cell-0f3dbb5dd4359cb0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Bag-of-words (BOW)\n",
    "\n",
    "BOW converts a sentence into an array of length `vocab_size`. Simply count the number of times a word appears within the sequence and increment the index according to its position within the vocabulary.\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color:black\"><b>Note:</b> The output of sklearn's CountVectorizer() is different to the numpy implementation. Can you work out why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: This is a test sentence which is a very long test sentence.\n",
      "\n",
      "Tokens: ['This', 'is', 'a', 'test', 'sentence', 'which', 'is', 'a', 'very', 'long', 'test', 'sentence', '.']\n",
      "\n",
      "Vocabulary: ['very', 'which', 'is', 'long', 'a', 'test', '.', 'supercalifragilisticexpialidocious', 'sentence', 'This']\n",
      "\n",
      "Token indices: [9, 2, 4, 5, 8, 1, 2, 4, 0, 3, 5, 8, 6]\n",
      "\n",
      "BOW with numpy:\n",
      " [1 1 2 1 2 2 1 0 2 1]\n",
      "\n",
      "BOW with sklearn:\n",
      " [[1 1 2 1 0 2 0 0 2 1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spacy document object\n",
    "text = \"This is a test sentence which is a very long test sentence.\"\n",
    "doc = nlp(text)\n",
    "print(f\"Document: {doc}\\n\")\n",
    "\n",
    "# Tokenise the document\n",
    "tokens = [token.text for token in doc]\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Create simple vocabulary (add word not in our input text)\n",
    "vocab = list(set(tokens + ['supercalifragilisticexpialidocious']))\n",
    "print(f\"Vocabulary: {vocab}\\n\")\n",
    "\n",
    "# Get a list of token indices within vocabulary\n",
    "token_indices = [vocab.index(token) for token in tokens]\n",
    "print(f\"Token indices: {token_indices}\\n\")\n",
    "\n",
    "# Create a BOW with numpy\n",
    "bow_np = np.zeros(len(vocab), dtype=np.int32)\n",
    "for i in range(len(token_indices)):\n",
    "    bow_np[token_indices[i]] += 1\n",
    "print(f\"BOW with numpy:\\n {bow_np}\\n\")\n",
    "\n",
    "# Create a BOW with sklearn\n",
    "bow_vectoriser = CountVectorizer(vocabulary=vocab, lowercase=False)\n",
    "bow_sk = bow_vectoriser.fit_transform([text])\n",
    "print(f\"BOW with sklearn:\\n {bow_sk.toarray()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "baf1550fd6cb952da51d47f9a6f76fdd",
     "grade": false,
     "grade_id": "cell-c9d68ec408354099",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF converts a corpus into an array of length `num_documents` x `vocab_size`. TF is the frequency of a word within a *document* and IDF is frequency of a word within the *corpus*. The TF-IDF for a word is then TF(word) x IDF(word):\n",
    "\n",
    "$w =$ word/term\n",
    "\n",
    "$d =$ document\n",
    "\n",
    "$N =$ number of documents in corpus\n",
    "\n",
    "$TF(w) = \\frac{count(w, d)}{len(d)}$\n",
    "\n",
    "$IDF(w) = log\\frac{N}{\\sum_{d=1}^{N} count(w, d) + 1}$\n",
    "\n",
    "$TF-IDF(w) = TF(w) \\times IDF(w)$\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color:black\"><b>Note:</b> The class TFIDF() mimics the sklearn implementation (as best as possible). Try different normalisations ('l1' or 'l2') and set smoothing True/False.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF with numpy:\n",
      "         car    driven   highway        is        on      road       the     truck\n",
      "0  0.201895  0.119242  0.000000  0.119242  0.119242  0.201895  0.238484  0.000000\n",
      "1  0.000000  0.119242  0.201895  0.119242  0.119242  0.000000  0.238484  0.201895\n",
      "\n",
      "        car    driven  highway        is   on  road       the  truck\n",
      "0  0.274156  0.181461      0.0  0.181461  0.0   0.0  0.362922    0.0\n",
      "\n",
      "TF-IDF with sklearn:\n",
      "         car    driven   highway        is        on      road       the     truck\n",
      "0  0.201895  0.119242  0.000000  0.119242  0.119242  0.201895  0.238484  0.000000\n",
      "1  0.000000  0.119242  0.201895  0.119242  0.119242  0.000000  0.238484  0.201895\n",
      "\n",
      "        car   driven  highway       is   on  road     the  truck\n",
      "0  0.297401  0.17565      0.0  0.17565  0.0   0.0  0.3513    0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TFIDF():\n",
    "    \"\"\"TF-IDF vectoriser.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokeniser=None, vocabulary=None, norm=None, smooth_idf=True):\n",
    "        \"\"\" Arguments:\n",
    "                tokeniser (callable): A function that takes a string and returns a list of tokens\n",
    "                vocabulary (list): A list of tokens to use as the vocabulary\n",
    "                norm (str): The normalisation to use when calculating the tf-idf vectors\n",
    "                smooth_idf (bool): Whether to use Laplace smoothing when calculating the idf\n",
    "        \"\"\"\n",
    "        self.corpus = None\n",
    "        self.N = None\n",
    "        self.tokeniser = tokeniser\n",
    "        self.vocabulary = vocabulary\n",
    "        self.norm = norm\n",
    "        self.smooth_idf = smooth_idf\n",
    "\n",
    "        if not self.tokeniser:\n",
    "            self.tokeniser = self._tokenise\n",
    "\n",
    "        # l1 norm is the sum of the absolute values of the vector\n",
    "        if self.norm and self.norm == 'l1':\n",
    "            self.norm = 1\n",
    "        # l2 norm is the square root of the sum of the squared values of the vector\n",
    "        elif self.norm and self.norm == 'l2':\n",
    "            self.norm = 2\n",
    "\n",
    "    def _tokenise(self, s):\n",
    "        return s.split()\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        vocab = []\n",
    "        for doc in self.corpus:\n",
    "            vocab.extend(self.tokeniser(doc))\n",
    "\n",
    "        vocab = list(set(vocab))\n",
    "        vocab.sort()\n",
    "        return vocab\n",
    "\n",
    "    def _tf(self):\n",
    "        \"\"\"Get the term frequency for each document in the corpus.\"\"\"\n",
    "\n",
    "        tf = []\n",
    "        for doc in self.corpus:\n",
    "            tf.append(Counter(self.tokeniser(doc)))\n",
    "        return tf\n",
    "\n",
    "    def _df(self):\n",
    "        \"\"\"Get the document frequency of each word in the corpus.\"\"\"\n",
    "\n",
    "        df = Counter()\n",
    "        for doc in self.corpus:\n",
    "            df.update(set(self.tokeniser(doc)))\n",
    "        return df\n",
    "\n",
    "    def _idf(self):\n",
    "        \"\"\"Calculate inverse document frequency for each word in the vocabulary.\"\"\"\n",
    "\n",
    "        # Calculate the DF\n",
    "        df = self._df()\n",
    "\n",
    "        idf = {}\n",
    "        for word in self.vocabulary:\n",
    "            if self.smooth_idf:\n",
    "                idf[word] = 1.0 + np.log((self.N + 1) / (df[word] + 1))\n",
    "            else:\n",
    "                idf[word] = 1.0 + np.log(np.divide(self.N, df[word]))\n",
    "        return idf\n",
    "\n",
    "    def _tfidf(self):\n",
    "        \"\"\"Calculate the TF-IDF for each document in the corpus.\"\"\"\n",
    "\n",
    "        # Calculate TF and IDF\n",
    "        tf = self._tf()\n",
    "        idf = self._idf()\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        tfidf = np.zeros((self.N, len(self.vocabulary)))\n",
    "\n",
    "        for i, doc in enumerate(self.corpus):\n",
    "            for j, word in enumerate(self.vocabulary):\n",
    "                tfidf[i, j] = tf[i][word] * idf[word]\n",
    "        \n",
    "        if self.norm:\n",
    "            tfidf = tfidf / np.linalg.norm(tfidf, ord=self.norm, axis=1, keepdims=True)\n",
    "        return tfidf\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        # Set corpus/N\n",
    "        self.corpus = np.array(corpus)\n",
    "        self.N = len(self.corpus)\n",
    "\n",
    "        # Set vocabulary\n",
    "        if not self.vocabulary:\n",
    "            self.vocabulary = self.get_vocabulary()\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        self.tfidf = self._tfidf()\n",
    "        return self\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        # Update corpus/N\n",
    "        self.corpus = np.append(self.corpus, corpus, axis=0)\n",
    "        self.N = len(self.corpus)\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        self.tfidf = self._tfidf()\n",
    "        return self.tfidf[-len(corpus):]\n",
    "\n",
    "corpus = ['the car is driven on the road', 'the truck is driven on the highway']\n",
    "\n",
    "# Create a TF-IDF with numpy\n",
    "tfidf_numpy = TFIDF(norm='l1', smooth_idf=False).fit(corpus)\n",
    "terms = tfidf_numpy.get_vocabulary()\n",
    "matrix_np = tfidf_numpy.transform(corpus)\n",
    "print(f\"TF-IDF with numpy:\\n {pd.DataFrame(data=matrix_np, columns=terms)}\\n\")\n",
    "\n",
    "# Transform a new sentence\n",
    "matrix_np = tfidf_numpy.transform(['the car is driven in the sky'])\n",
    "print(f\"{pd.DataFrame(data=matrix_np, columns=terms)}\\n\")\n",
    "\n",
    "# Create a TF-IDF with sklearn\n",
    "tfidf_sklearn = TfidfVectorizer(norm='l1', smooth_idf=False).fit(corpus)\n",
    "terms = tfidf_sklearn.get_feature_names_out()\n",
    "matrix_sk = tfidf_sklearn.transform(corpus).toarray()\n",
    "print(f\"TF-IDF with sklearn:\\n {pd.DataFrame(data=matrix_sk, columns=terms)}\\n\")\n",
    "\n",
    "# Transform a new sentence\n",
    "matrix_sk = tfidf_sklearn.transform(['the car is driven in the sky']).toarray()\n",
    "print(f\"{pd.DataFrame(data=matrix_sk, columns=terms)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c97a6e4461aabe49ecd79aa23bfd3f6e",
     "grade": false,
     "grade_id": "cell-94597c9b71db32b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### N-grams\n",
    "\n",
    "N-grams are sequences of N words. Typically uni-grams (1), bi-grams (2) and tri-grams (3). Bi-grams and tri-grams (or larger) provide some context to words and can be used as replacement for uni-grams in many models. Here we use NLTK to create tuples of all bi-grams and tri-grams from the text.\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color:black\"><b>Note:</b> The sklearn CountVectorizer() and TfidfVectorizer() have an <code>ngram_range</code> argument which allows you to vectorise N-grams instead of single words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: I sat by the riverbank. I went to the bank to withdraw money.\n",
      "\n",
      "Sentence: I sat by the riverbank.\n",
      "Bi-grams: [('I', 'sat'), ('sat', 'by'), ('by', 'the'), ('the', 'riverbank'), ('riverbank', '.')]\n",
      "Tri-grams: [('I', 'sat', 'by'), ('sat', 'by', 'the'), ('by', 'the', 'riverbank'), ('the', 'riverbank', '.')]\n",
      "\n",
      "Sentence: I went to the bank to withdraw money.\n",
      "Bi-grams: [('I', 'went'), ('went', 'to'), ('to', 'the'), ('the', 'bank'), ('bank', 'to'), ('to', 'withdraw'), ('withdraw', 'money'), ('money', '.')]\n",
      "Tri-grams: [('I', 'went', 'to'), ('went', 'to', 'the'), ('to', 'the', 'bank'), ('the', 'bank', 'to'), ('bank', 'to', 'withdraw'), ('to', 'withdraw', 'money'), ('withdraw', 'money', '.')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spacy document object\n",
    "text = 'I sat by the riverbank. I went to the bank to withdraw money.'\n",
    "doc = nlp(text)\n",
    "print(f\"Document: {doc}\\n\")\n",
    "\n",
    "# Create N-grams with nltk\n",
    "for sent in doc.sents:\n",
    "    print(f\"Sentence: {sent}\")\n",
    "\n",
    "    tokens = [token.text for token in sent]\n",
    "\n",
    "    bi_grams = list(ngrams(tokens, 2))\n",
    "    print(f\"Bi-grams: {bi_grams}\")\n",
    "\n",
    "    tri_grams = list(ngrams(tokens, 3))\n",
    "    print(f\"Tri-grams: {tri_grams}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f62516b7f5bf733c4d7901873cdc5c07",
     "grade": false,
     "grade_id": "cell-aff8c261ff6559ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Word Vectors\n",
    "\n",
    "Word vectors represent single words as a vector (list) of real numbers which capture some aspect of their meaning and relationships to other words. The best known (and first) method is [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf) which uses either skip-gram (given context word predict surrounding target words), or a continuous bag of words (predict target word given context words). Word vector models are typically trained on 100's of millions of words to produce a set of weights - an embedding matrix - of shape `vocab_size` x `embedding_dim`, where the embedding dimension is the length of a vector for each word (usually 50 to 300).\n",
    "\n",
    "Once trained these embeddings can be used as semantically rich word representations for other NLP tasks, such as classification. This is called *transfer learning*, where the weights for a model trained on one objective (predicting words) can be used as input to train models on a different task (classification, language modelling, etc). There are lots of pre-trained word vectors available to download which can be used to map words to vectors for input into your models.\n",
    "\n",
    "Spacy comes with pre-trained 300 dimensional word vectors, so it is easy to create a document and get the word vector for each token.\n",
    "\n",
    "With word vectors we can also use cosine similarity, which is a measure of similarity between two sequences of numbers, to calculate a similarity score in the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: dog cat banana apple fish\n",
      "\n",
      "dog vector: [  1.233     4.2963   -7.9738  -10.121     1.8207    1.4098   -4.518\n",
      "  -5.2261   -0.29157   0.95234] shape: (300,)\n",
      "\n",
      "cat vector: [  3.7032     4.1982    -5.0002   -11.322      0.031702  -1.0255\n",
      "  -3.087     -3.7327     0.53875    3.5679  ] shape: (300,)\n",
      "\n",
      "banana vector: [ 0.20778 -2.4151   0.36605  2.0139  -0.23752 -3.1952  -0.2952   1.2272\n",
      " -3.4129  -0.54969] shape: (300,)\n",
      "\n",
      "apple vector: [-1.0084  -2.0308  -0.64185  2.6928   0.31771 -2.6662  -3.7372   5.4714\n",
      " -5.1751   0.51958] shape: (300,)\n",
      "\n",
      "fish vector: [-1.1278  -4.2107  -4.1088   0.73152  3.3726  -2.538   -1.8874   4.4615\n",
      " -5.8596   1.8804 ] shape: (300,)\n",
      "\n",
      "Similarity between 'dog' and 'cat': 0.8220816850662231\n",
      "\n",
      "Similarity between 'dog' and 'banana': 0.2090904712677002\n",
      "\n",
      "Similarity between 'I like fish and chips.' and 'I like cats and dogs.': 0.8804120795837534\n",
      "\n",
      "Similarity between 'I like fish and chips.' and 'NLP, it's fun!.': 0.6635456799293546\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create spacy document object\n",
    "raw_text = \"dog cat banana apple fish\"\n",
    "doc = nlp(raw_text)\n",
    "print(f\"Document: {doc}\\n\")\n",
    "\n",
    "# Print the word, its vectors (first 10 dimensions) and size of vector\n",
    "for token in doc:\n",
    "    print(F\"{token.text} vector: {token.vector[:10]} shape: {token.vector.shape}\\n\")\n",
    "\n",
    "# Print the similarity between two words\n",
    "print(f\"Similarity between '{doc[0].text}' and '{doc[1].text}': {doc[0].similarity(doc[1])}\\n\")\n",
    "print(f\"Similarity between '{doc[0].text}' and '{doc[2].text}': {doc[0].similarity(doc[2])}\\n\")\n",
    "\n",
    "# Similarity of two documents\n",
    "doc1 = nlp(\"I like fish and chips.\")\n",
    "doc2 = nlp(\"I like cats and dogs.\")\n",
    "doc3 = nlp(\"NLP, it's fun!.\")\n",
    "\n",
    "# Print the similarity between two documents\n",
    "print(f\"Similarity between '{doc1}' and '{doc2}': {doc1.similarity(doc2)}\\n\")\n",
    "print(f\"Similarity between '{doc1}' and '{doc3}': {doc1.similarity(doc3)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff838db2b941037d05b6641acb4021e2",
     "grade": false,
     "grade_id": "cell-99e8ceadcf5425cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Classic $King - Man + Woman \\approx Queen$ example.\n",
    "\n",
    "<div class=\"alert alert-warning\" style=\"color:black\"><b>Bias in word embeddings:</b> As with many problems in machine learning, the models we train tend to pick up the underlying biases within the data we train them on. Word embeddings are no different, and often reflect the human biases present within the huge amounts of text data they were trained on.\n",
    "\n",
    "This is illustrated by Nissim, M., et al. (2020), who provide example analogies like *\"man is to computer programmer as woman is to homemaker\"*. They also argue that these biases, when present in embeddings, could be propagated outside of NLP and AI into other domains, misleading those who are less well equipped to understand, or even be aware of the presence of such bias.\n",
    "\n",
    "*Nissim, M., et al. (2020) Fair Is Better than Sensational: Man Is to Doctor as Woman Is to Doctor. Computational Linguistics. 46 (2), pp. 487â497. Available from: https://aclanthology.org/2020.cl-2.7/.*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Similarities:\n",
      "\n",
      "queen similarity: 0.6178013682365417\n",
      "\n",
      "king similarity: 0.8489542603492737\n",
      "\n",
      "woman similarity: 0.3099472224712372\n",
      "\n",
      "man similarity: 0.07003619521856308\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the vectors for each word\n",
    "doc = nlp('queen king woman man')\n",
    "queen, king, woman, man = doc[0].vector, doc[1].vector, doc[2].vector, doc[3].vector\n",
    "\n",
    "# Perform vector arithmetic\n",
    "new_vec = king - man + woman\n",
    "\n",
    "# Find the most similar word to the new vector\n",
    "print(\"Word Similarities:\\n\")\n",
    "for word in doc:\n",
    "    sim = cosine_similarity(np.expand_dims(word.vector, axis=0), np.expand_dims(new_vec, axis=0))\n",
    "    print(f\"{word} similarity: {sim[0][0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee1fba50a79b8ef0eb5048884bac53a4",
     "grade": false,
     "grade_id": "cell-2684b0d7d9128232",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.2 Visualising word vectors\n",
    "\n",
    "A common use-case for word vectors is to create an embedding matrix, which is used as a lookup table to map words to their vector representations. Here we will do this with the IMDB reviews that we have annotated and processed. The resulting embedding matrix will have shape `vocab_size` x `embedding_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4432\n",
      "\n",
      "Embedding dataframe shape:\n",
      " (4432, 300)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>there</th>\n",
       "      <td>1.09230</td>\n",
       "      <td>4.454600</td>\n",
       "      <td>-2.83500</td>\n",
       "      <td>2.184600</td>\n",
       "      <td>-0.30957</td>\n",
       "      <td>1.26770</td>\n",
       "      <td>-0.041528</td>\n",
       "      <td>8.64330</td>\n",
       "      <td>-2.44810</td>\n",
       "      <td>2.743600</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00410</td>\n",
       "      <td>-2.75860</td>\n",
       "      <td>4.98920</td>\n",
       "      <td>-4.319500</td>\n",
       "      <td>-1.91440</td>\n",
       "      <td>0.746080</td>\n",
       "      <td>-1.9334</td>\n",
       "      <td>1.47510</td>\n",
       "      <td>-3.56240</td>\n",
       "      <td>1.72340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1.47500</td>\n",
       "      <td>6.007800</td>\n",
       "      <td>1.12050</td>\n",
       "      <td>-3.587400</td>\n",
       "      <td>3.76380</td>\n",
       "      <td>3.19870</td>\n",
       "      <td>-2.206000</td>\n",
       "      <td>3.21280</td>\n",
       "      <td>-2.08160</td>\n",
       "      <td>-0.002931</td>\n",
       "      <td>...</td>\n",
       "      <td>10.95500</td>\n",
       "      <td>-2.96190</td>\n",
       "      <td>4.54070</td>\n",
       "      <td>-2.299900</td>\n",
       "      <td>-0.99536</td>\n",
       "      <td>1.261900</td>\n",
       "      <td>-2.3326</td>\n",
       "      <td>-0.22893</td>\n",
       "      <td>-0.85967</td>\n",
       "      <td>9.74660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-9.36290</td>\n",
       "      <td>9.276100</td>\n",
       "      <td>-7.27080</td>\n",
       "      <td>4.387900</td>\n",
       "      <td>10.31600</td>\n",
       "      <td>-6.84690</td>\n",
       "      <td>1.575500</td>\n",
       "      <td>7.94050</td>\n",
       "      <td>8.08120</td>\n",
       "      <td>2.619400</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.67110</td>\n",
       "      <td>3.60260</td>\n",
       "      <td>0.94914</td>\n",
       "      <td>5.986100</td>\n",
       "      <td>0.14368</td>\n",
       "      <td>9.706600</td>\n",
       "      <td>4.4738</td>\n",
       "      <td>2.68010</td>\n",
       "      <td>-6.81600</td>\n",
       "      <td>3.57370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>2.59300</td>\n",
       "      <td>4.345400</td>\n",
       "      <td>-1.46600</td>\n",
       "      <td>-1.265600</td>\n",
       "      <td>2.87050</td>\n",
       "      <td>-1.04540</td>\n",
       "      <td>3.917600</td>\n",
       "      <td>4.81050</td>\n",
       "      <td>-4.48850</td>\n",
       "      <td>3.240100</td>\n",
       "      <td>...</td>\n",
       "      <td>2.04450</td>\n",
       "      <td>4.75350</td>\n",
       "      <td>-2.76160</td>\n",
       "      <td>-0.204980</td>\n",
       "      <td>1.42460</td>\n",
       "      <td>-2.583400</td>\n",
       "      <td>-4.1083</td>\n",
       "      <td>-1.26100</td>\n",
       "      <td>-5.44780</td>\n",
       "      <td>4.28850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuclear</th>\n",
       "      <td>3.21590</td>\n",
       "      <td>-1.182400</td>\n",
       "      <td>-0.65593</td>\n",
       "      <td>0.958640</td>\n",
       "      <td>7.40440</td>\n",
       "      <td>-0.60504</td>\n",
       "      <td>1.087600</td>\n",
       "      <td>3.72380</td>\n",
       "      <td>-0.85234</td>\n",
       "      <td>-1.455200</td>\n",
       "      <td>...</td>\n",
       "      <td>3.48660</td>\n",
       "      <td>-3.94360</td>\n",
       "      <td>-1.44540</td>\n",
       "      <td>-0.934290</td>\n",
       "      <td>-1.85500</td>\n",
       "      <td>-2.107700</td>\n",
       "      <td>3.7509</td>\n",
       "      <td>2.50100</td>\n",
       "      <td>-1.17800</td>\n",
       "      <td>2.42270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arms</th>\n",
       "      <td>0.33463</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>-5.56220</td>\n",
       "      <td>3.172900</td>\n",
       "      <td>6.61350</td>\n",
       "      <td>2.37280</td>\n",
       "      <td>-2.585700</td>\n",
       "      <td>2.00950</td>\n",
       "      <td>3.40460</td>\n",
       "      <td>2.661200</td>\n",
       "      <td>...</td>\n",
       "      <td>1.09590</td>\n",
       "      <td>1.19040</td>\n",
       "      <td>5.59690</td>\n",
       "      <td>-1.753100</td>\n",
       "      <td>-1.90940</td>\n",
       "      <td>0.044897</td>\n",
       "      <td>-2.0561</td>\n",
       "      <td>3.28930</td>\n",
       "      <td>-2.90750</td>\n",
       "      <td>0.23962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>-4.38980</td>\n",
       "      <td>0.362130</td>\n",
       "      <td>-2.75860</td>\n",
       "      <td>1.539900</td>\n",
       "      <td>5.00260</td>\n",
       "      <td>2.83170</td>\n",
       "      <td>5.876200</td>\n",
       "      <td>12.02400</td>\n",
       "      <td>-1.35220</td>\n",
       "      <td>-0.395410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.56888</td>\n",
       "      <td>2.28880</td>\n",
       "      <td>-3.08190</td>\n",
       "      <td>0.078426</td>\n",
       "      <td>-1.57720</td>\n",
       "      <td>0.554510</td>\n",
       "      <td>4.1041</td>\n",
       "      <td>1.55800</td>\n",
       "      <td>-1.92940</td>\n",
       "      <td>-3.18730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>underway</th>\n",
       "      <td>1.91330</td>\n",
       "      <td>1.652500</td>\n",
       "      <td>-0.98616</td>\n",
       "      <td>0.805400</td>\n",
       "      <td>2.70300</td>\n",
       "      <td>0.88577</td>\n",
       "      <td>1.511300</td>\n",
       "      <td>2.75560</td>\n",
       "      <td>1.49920</td>\n",
       "      <td>2.511600</td>\n",
       "      <td>...</td>\n",
       "      <td>1.71430</td>\n",
       "      <td>1.97350</td>\n",
       "      <td>0.51199</td>\n",
       "      <td>-1.648300</td>\n",
       "      <td>-3.05460</td>\n",
       "      <td>1.486200</td>\n",
       "      <td>2.0454</td>\n",
       "      <td>-3.30490</td>\n",
       "      <td>1.51300</td>\n",
       "      <td>2.00620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>superman</th>\n",
       "      <td>-2.27880</td>\n",
       "      <td>0.875930</td>\n",
       "      <td>1.47470</td>\n",
       "      <td>-0.028177</td>\n",
       "      <td>1.58890</td>\n",
       "      <td>-0.44738</td>\n",
       "      <td>1.000500</td>\n",
       "      <td>0.58913</td>\n",
       "      <td>2.46360</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.94490</td>\n",
       "      <td>-3.01500</td>\n",
       "      <td>-0.55688</td>\n",
       "      <td>1.696800</td>\n",
       "      <td>1.12840</td>\n",
       "      <td>1.831600</td>\n",
       "      <td>2.1094</td>\n",
       "      <td>-1.79080</td>\n",
       "      <td>1.22690</td>\n",
       "      <td>-0.41504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forbidden</th>\n",
       "      <td>-0.83916</td>\n",
       "      <td>0.568580</td>\n",
       "      <td>0.62002</td>\n",
       "      <td>0.246290</td>\n",
       "      <td>2.55950</td>\n",
       "      <td>0.33772</td>\n",
       "      <td>2.650700</td>\n",
       "      <td>-0.62044</td>\n",
       "      <td>-0.65833</td>\n",
       "      <td>0.609810</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.72370</td>\n",
       "      <td>0.90849</td>\n",
       "      <td>0.88191</td>\n",
       "      <td>-4.893300</td>\n",
       "      <td>0.10066</td>\n",
       "      <td>-1.915900</td>\n",
       "      <td>-1.9116</td>\n",
       "      <td>-0.51103</td>\n",
       "      <td>-2.03170</td>\n",
       "      <td>-0.30917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1        2         3         4        5         6         7        8         9    ...       290      291      292       293      294       295     296      297      298      299\n",
       "there      1.09230  4.454600 -2.83500  2.184600  -0.30957  1.26770 -0.041528   8.64330 -2.44810  2.743600  ...   2.00410 -2.75860  4.98920 -4.319500 -1.91440  0.746080 -1.9334  1.47510 -3.56240  1.72340\n",
       "is         1.47500  6.007800  1.12050 -3.587400   3.76380  3.19870 -2.206000   3.21280 -2.08160 -0.002931  ...  10.95500 -2.96190  4.54070 -2.299900 -0.99536  1.261900 -2.3326 -0.22893 -0.85967  9.74660\n",
       "a         -9.36290  9.276100 -7.27080  4.387900  10.31600 -6.84690  1.575500   7.94050  8.08120  2.619400  ...  -8.67110  3.60260  0.94914  5.986100  0.14368  9.706600  4.4738  2.68010 -6.81600  3.57370\n",
       "new        2.59300  4.345400 -1.46600 -1.265600   2.87050 -1.04540  3.917600   4.81050 -4.48850  3.240100  ...   2.04450  4.75350 -2.76160 -0.204980  1.42460 -2.583400 -4.1083 -1.26100 -5.44780  4.28850\n",
       "nuclear    3.21590 -1.182400 -0.65593  0.958640   7.40440 -0.60504  1.087600   3.72380 -0.85234 -1.455200  ...   3.48660 -3.94360 -1.44540 -0.934290 -1.85500 -2.107700  3.7509  2.50100 -1.17800  2.42270\n",
       "arms       0.33463  0.007916 -5.56220  3.172900   6.61350  2.37280 -2.585700   2.00950  3.40460  2.661200  ...   1.09590  1.19040  5.59690 -1.753100 -1.90940  0.044897 -2.0561  3.28930 -2.90750  0.23962\n",
       "race      -4.38980  0.362130 -2.75860  1.539900   5.00260  2.83170  5.876200  12.02400 -1.35220 -0.395410  ...   0.56888  2.28880 -3.08190  0.078426 -1.57720  0.554510  4.1041  1.55800 -1.92940 -3.18730\n",
       "underway   1.91330  1.652500 -0.98616  0.805400   2.70300  0.88577  1.511300   2.75560  1.49920  2.511600  ...   1.71430  1.97350  0.51199 -1.648300 -3.05460  1.486200  2.0454 -3.30490  1.51300  2.00620\n",
       "superman  -2.27880  0.875930  1.47470 -0.028177   1.58890 -0.44738  1.000500   0.58913  2.46360  1.330000  ...   0.94490 -3.01500 -0.55688  1.696800  1.12840  1.831600  2.1094 -1.79080  1.22690 -0.41504\n",
       "forbidden -0.83916  0.568580  0.62002  0.246290   2.55950  0.33772  2.650700  -0.62044 -0.65833  0.609810  ...  -1.72370  0.90849  0.88191 -4.893300  0.10066 -1.915900 -1.9116 -0.51103 -2.03170 -0.30917\n",
       "\n",
       "[10 rows x 300 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the imdb reviews\n",
    "imdb_reviews = pd.read_csv(os.path.join(data_dir, 'imdb_reviews.csv'))\n",
    "\n",
    "# Tokenise the corpus\n",
    "imdb_corpus = imdb_reviews['review'].apply(lambda x: [token.text for token in nlp.tokenizer(x)])\n",
    "\n",
    "# Create simple vocabulary\n",
    "imdb_vocab = imdb_corpus.explode().unique().tolist()\n",
    "print(f\"Vocabulary size: {len(imdb_vocab)}\\n\")\n",
    "\n",
    "# Set the dimensionality of the word vectors\n",
    "embedding_dim = 300\n",
    "\n",
    "# Create an empty numpy array\n",
    "embedding_matrix = np.zeros((len(imdb_vocab), embedding_dim))\n",
    "\n",
    "# For each word in the imdb vocabulary\n",
    "for i, word in enumerate(imdb_vocab):\n",
    "    # If the word has a vector\n",
    "    if nlp.vocab.has_vector(word):\n",
    "        # Get the vector for the word\n",
    "        embedding_matrix[i] = nlp.vocab[word].vector\n",
    "    else:\n",
    "        # Get a random vector\n",
    "        embedding_matrix[i] = np.random.uniform(np.min(embedding_matrix), np.max(embedding_matrix), embedding_dim)\n",
    "\n",
    "# Create dataframe with words and vectors\n",
    "embedding_df = pd.DataFrame(embedding_matrix, index=imdb_vocab)\n",
    "print(f\"Embedding dataframe shape:\\n {embedding_df.shape}\\n\")\n",
    "embedding_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4174f581154afc3f0713a2832d324689",
     "grade": false,
     "grade_id": "cell-65871a2b48b6d6ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Calculate the similarity between all words in the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>there</th>\n",
       "      <th>is</th>\n",
       "      <th>a</th>\n",
       "      <th>new</th>\n",
       "      <th>nuclear</th>\n",
       "      <th>arms</th>\n",
       "      <th>race</th>\n",
       "      <th>underway</th>\n",
       "      <th>superman</th>\n",
       "      <th>forbidden</th>\n",
       "      <th>...</th>\n",
       "      <th>beauty</th>\n",
       "      <th>sludge</th>\n",
       "      <th>mixture</th>\n",
       "      <th>conflict</th>\n",
       "      <th>boorman</th>\n",
       "      <th>grasp</th>\n",
       "      <th>casablanca</th>\n",
       "      <th>vertigo</th>\n",
       "      <th>r</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>word_ind</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>there</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185102</td>\n",
       "      <td>0.268007</td>\n",
       "      <td>0.224541</td>\n",
       "      <td>0.328851</td>\n",
       "      <td>0.185183</td>\n",
       "      <td>0.189985</td>\n",
       "      <td>0.435817</td>\n",
       "      <td>0.056798</td>\n",
       "      <td>0.360909</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254705</td>\n",
       "      <td>0.200327</td>\n",
       "      <td>0.319161</td>\n",
       "      <td>0.426701</td>\n",
       "      <td>-0.022115</td>\n",
       "      <td>0.277568</td>\n",
       "      <td>0.111729</td>\n",
       "      <td>0.412712</td>\n",
       "      <td>0.057114</td>\n",
       "      <td>-0.083122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>is</th>\n",
       "      <td>0.185102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289764</td>\n",
       "      <td>0.047177</td>\n",
       "      <td>0.158434</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.083540</td>\n",
       "      <td>0.081484</td>\n",
       "      <td>0.017270</td>\n",
       "      <td>0.112925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205849</td>\n",
       "      <td>0.036951</td>\n",
       "      <td>0.213609</td>\n",
       "      <td>0.184534</td>\n",
       "      <td>-0.043874</td>\n",
       "      <td>0.300133</td>\n",
       "      <td>-0.004319</td>\n",
       "      <td>0.174874</td>\n",
       "      <td>0.059598</td>\n",
       "      <td>-0.055503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>a</th>\n",
       "      <td>0.268007</td>\n",
       "      <td>0.289764</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.255128</td>\n",
       "      <td>0.220140</td>\n",
       "      <td>0.172469</td>\n",
       "      <td>0.122219</td>\n",
       "      <td>0.215751</td>\n",
       "      <td>0.103133</td>\n",
       "      <td>0.047692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187148</td>\n",
       "      <td>0.065131</td>\n",
       "      <td>0.242395</td>\n",
       "      <td>0.213781</td>\n",
       "      <td>-0.058815</td>\n",
       "      <td>0.168449</td>\n",
       "      <td>0.124677</td>\n",
       "      <td>0.222516</td>\n",
       "      <td>0.066532</td>\n",
       "      <td>-0.062982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>new</th>\n",
       "      <td>0.224541</td>\n",
       "      <td>0.047177</td>\n",
       "      <td>0.255128</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.263856</td>\n",
       "      <td>0.065058</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>0.336416</td>\n",
       "      <td>0.122601</td>\n",
       "      <td>0.129612</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229109</td>\n",
       "      <td>0.116252</td>\n",
       "      <td>0.198524</td>\n",
       "      <td>0.198738</td>\n",
       "      <td>-0.009455</td>\n",
       "      <td>0.081388</td>\n",
       "      <td>0.093970</td>\n",
       "      <td>0.122860</td>\n",
       "      <td>0.050540</td>\n",
       "      <td>-0.028430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>nuclear</th>\n",
       "      <td>0.328851</td>\n",
       "      <td>0.158434</td>\n",
       "      <td>0.220140</td>\n",
       "      <td>0.263856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.306704</td>\n",
       "      <td>0.139159</td>\n",
       "      <td>0.450367</td>\n",
       "      <td>0.239214</td>\n",
       "      <td>0.205328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063567</td>\n",
       "      <td>0.207092</td>\n",
       "      <td>0.239058</td>\n",
       "      <td>0.492736</td>\n",
       "      <td>-0.034024</td>\n",
       "      <td>0.099839</td>\n",
       "      <td>0.189140</td>\n",
       "      <td>0.166196</td>\n",
       "      <td>-0.034553</td>\n",
       "      <td>-0.045302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>arms</th>\n",
       "      <td>0.185183</td>\n",
       "      <td>0.004547</td>\n",
       "      <td>0.172469</td>\n",
       "      <td>0.065058</td>\n",
       "      <td>0.306704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.171109</td>\n",
       "      <td>0.163449</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.193794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096928</td>\n",
       "      <td>-0.022480</td>\n",
       "      <td>0.167853</td>\n",
       "      <td>0.269723</td>\n",
       "      <td>-0.075271</td>\n",
       "      <td>0.157007</td>\n",
       "      <td>0.196318</td>\n",
       "      <td>0.200296</td>\n",
       "      <td>-0.052197</td>\n",
       "      <td>-0.063862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>race</th>\n",
       "      <td>0.189985</td>\n",
       "      <td>0.083540</td>\n",
       "      <td>0.122219</td>\n",
       "      <td>0.055729</td>\n",
       "      <td>0.139159</td>\n",
       "      <td>0.171109</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194672</td>\n",
       "      <td>0.110399</td>\n",
       "      <td>0.095752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097606</td>\n",
       "      <td>0.031124</td>\n",
       "      <td>0.144889</td>\n",
       "      <td>0.282314</td>\n",
       "      <td>-0.034714</td>\n",
       "      <td>0.106489</td>\n",
       "      <td>0.181087</td>\n",
       "      <td>0.121760</td>\n",
       "      <td>0.025867</td>\n",
       "      <td>-0.016325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>underway</th>\n",
       "      <td>0.435817</td>\n",
       "      <td>0.081484</td>\n",
       "      <td>0.215751</td>\n",
       "      <td>0.336416</td>\n",
       "      <td>0.450367</td>\n",
       "      <td>0.163449</td>\n",
       "      <td>0.194672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.175782</td>\n",
       "      <td>0.159313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083011</td>\n",
       "      <td>0.277957</td>\n",
       "      <td>0.192092</td>\n",
       "      <td>0.353972</td>\n",
       "      <td>-0.157315</td>\n",
       "      <td>0.091973</td>\n",
       "      <td>0.203828</td>\n",
       "      <td>0.264346</td>\n",
       "      <td>-0.060568</td>\n",
       "      <td>-0.025443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>superman</th>\n",
       "      <td>0.056798</td>\n",
       "      <td>0.017270</td>\n",
       "      <td>0.103133</td>\n",
       "      <td>0.122601</td>\n",
       "      <td>0.239214</td>\n",
       "      <td>0.064000</td>\n",
       "      <td>0.110399</td>\n",
       "      <td>0.175782</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055095</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025244</td>\n",
       "      <td>0.041558</td>\n",
       "      <td>0.007592</td>\n",
       "      <td>0.195978</td>\n",
       "      <td>-0.028031</td>\n",
       "      <td>-0.012031</td>\n",
       "      <td>0.144935</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>-0.086127</td>\n",
       "      <td>-0.025004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <th>forbidden</th>\n",
       "      <td>0.360909</td>\n",
       "      <td>0.112925</td>\n",
       "      <td>0.047692</td>\n",
       "      <td>0.129612</td>\n",
       "      <td>0.205328</td>\n",
       "      <td>0.193794</td>\n",
       "      <td>0.095752</td>\n",
       "      <td>0.159313</td>\n",
       "      <td>0.055095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221911</td>\n",
       "      <td>0.099580</td>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.286433</td>\n",
       "      <td>-0.034382</td>\n",
       "      <td>0.161164</td>\n",
       "      <td>0.233923</td>\n",
       "      <td>0.083684</td>\n",
       "      <td>0.067220</td>\n",
       "      <td>-0.083448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã 4432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                there        is         a       new   nuclear      arms      race  underway  superman  forbidden  ...    beauty    sludge   mixture  conflict   boorman     grasp  casablanca   vertigo         r         v\n",
       "  word_ind                                                                                                        ...                                                                                                      \n",
       "0 there      1.000000  0.185102  0.268007  0.224541  0.328851  0.185183  0.189985  0.435817  0.056798   0.360909  ...  0.254705  0.200327  0.319161  0.426701 -0.022115  0.277568    0.111729  0.412712  0.057114 -0.083122\n",
       "1 is         0.185102  1.000000  0.289764  0.047177  0.158434  0.004547  0.083540  0.081484  0.017270   0.112925  ...  0.205849  0.036951  0.213609  0.184534 -0.043874  0.300133   -0.004319  0.174874  0.059598 -0.055503\n",
       "2 a          0.268007  0.289764  1.000000  0.255128  0.220140  0.172469  0.122219  0.215751  0.103133   0.047692  ...  0.187148  0.065131  0.242395  0.213781 -0.058815  0.168449    0.124677  0.222516  0.066532 -0.062982\n",
       "3 new        0.224541  0.047177  0.255128  1.000000  0.263856  0.065058  0.055729  0.336416  0.122601   0.129612  ...  0.229109  0.116252  0.198524  0.198738 -0.009455  0.081388    0.093970  0.122860  0.050540 -0.028430\n",
       "4 nuclear    0.328851  0.158434  0.220140  0.263856  1.000000  0.306704  0.139159  0.450367  0.239214   0.205328  ...  0.063567  0.207092  0.239058  0.492736 -0.034024  0.099839    0.189140  0.166196 -0.034553 -0.045302\n",
       "5 arms       0.185183  0.004547  0.172469  0.065058  0.306704  1.000000  0.171109  0.163449  0.064000   0.193794  ...  0.096928 -0.022480  0.167853  0.269723 -0.075271  0.157007    0.196318  0.200296 -0.052197 -0.063862\n",
       "6 race       0.189985  0.083540  0.122219  0.055729  0.139159  0.171109  1.000000  0.194672  0.110399   0.095752  ...  0.097606  0.031124  0.144889  0.282314 -0.034714  0.106489    0.181087  0.121760  0.025867 -0.016325\n",
       "7 underway   0.435817  0.081484  0.215751  0.336416  0.450367  0.163449  0.194672  1.000000  0.175782   0.159313  ...  0.083011  0.277957  0.192092  0.353972 -0.157315  0.091973    0.203828  0.264346 -0.060568 -0.025443\n",
       "8 superman   0.056798  0.017270  0.103133  0.122601  0.239214  0.064000  0.110399  0.175782  1.000000   0.055095  ... -0.025244  0.041558  0.007592  0.195978 -0.028031 -0.012031    0.144935  0.040963 -0.086127 -0.025004\n",
       "9 forbidden  0.360909  0.112925  0.047692  0.129612  0.205328  0.193794  0.095752  0.159313  0.055095   1.000000  ...  0.221911  0.099580  0.104553  0.286433 -0.034382  0.161164    0.233923  0.083684  0.067220 -0.083448\n",
       "\n",
       "[10 rows x 4432 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the cosine similarity between the words\n",
    "similarity_matrix = cosine_similarity(embedding_df)\n",
    "# Create dataframe with words and similarity\n",
    "similarity_df = pd.DataFrame(similarity_matrix, columns=imdb_vocab)\n",
    "# Add word as second index\n",
    "similarity_df.insert(0, 'word_ind', imdb_vocab)\n",
    "similarity_df.set_index('word_ind', inplace=True, append=True)\n",
    "similarity_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a731ca353d8ed7bedfa2b092442c7773",
     "grade": false,
     "grade_id": "cell-6adbf2a1df613381",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can use the vectors to visualise the most similar and least similar words to a given target word.\n",
    "\n",
    "1. We will use [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to reduce the dimensionality of the embeddings so we can visualise them.\n",
    "\n",
    "2. Next find the N most similar and dissimilar words to a target word.\n",
    "\n",
    "3. Create a 3D plot of the embeddings. With `N=5` and 'reviewers' you should see that, for example, 'review' and 'critics' are very close in the embedding (vector) space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'reviewers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reviewers'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m pca_embeddings \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mfit_transform(embedding_matrix)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Find the N most/least similar words\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m most_sim \u001b[38;5;241m=\u001b[39m \u001b[43msimilarity_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m:N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m least_sim \u001b[38;5;241m=\u001b[39m similarity_df[word]\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m:N \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m most_sim_words \u001b[38;5;241m=\u001b[39m [w \u001b[38;5;28;01mfor\u001b[39;00m ind, w \u001b[38;5;129;01min\u001b[39;00m most_sim\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'reviewers'"
     ]
    }
   ],
   "source": [
    "# Set the number of similar/dissimilar words and a target word\n",
    "N = 5\n",
    "word = 'reviewers'\n",
    "\n",
    "# Perform PCA (dimensionality reduction) on the embedding matrix\n",
    "pca_embeddings = PCA(n_components=3).fit_transform(embedding_matrix)\n",
    "\n",
    "# Find the N most/least similar words\n",
    "most_sim = similarity_df[word].sort_values(ascending=False)[0:N + 1]\n",
    "least_sim = similarity_df[word].sort_values(ascending=True)[0:N + 1]\n",
    "\n",
    "most_sim_words = [w for ind, w in most_sim.index.values]\n",
    "least_sim_words = [w for ind, w in least_sim.index.values]\n",
    "\n",
    "# Get the indices of the most/least similar words from the reduced embedding matrix\n",
    "most_sim_pca = pca_embeddings[[ind for ind, w in most_sim.index.values]]\n",
    "least_sim_pca = pca_embeddings[[ind for ind, w in least_sim.index.values]]\n",
    "\n",
    "# Plot the most/least similar words\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(most_sim_pca[:, 0], most_sim_pca[:, 1],  most_sim_pca[:, 2], linewidths=1, color='blue')\n",
    "ax.scatter(least_sim_pca[:, 0], least_sim_pca[:, 1],  least_sim_pca[:, 2], linewidths=1, color='red')\n",
    "# Add words to the plot\n",
    "for i, word in enumerate(most_sim_words):\n",
    "    ax.text(most_sim_pca[i, 0]+.02, most_sim_pca[i, 1], most_sim_pca[i, 2], word, size=10, zorder=1)\n",
    "for i, word in enumerate(least_sim_words):\n",
    "    ax.text(least_sim_pca[i, 0]+.02, least_sim_pca[i, 1], least_sim_pca[i, 2], word, size=10, zorder=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c080bfbb630d934a02137a61f25abfaf",
     "grade": false,
     "grade_id": "cell-1c83b6f373884018",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color:black\"><h3>Before you submit this notebook to NBGrader for marking:</h3> \n",
    "\n",
    "1. Make sure have completed all exercises marked by <span style=\"color:blue\">**blue cells**</span>.\n",
    "2. For automatically marked exercises ensure you have completed any cells with `# YOUR CODE HERE`. Then click 'Validate' button above, or ensure all cells run without producing an error.\n",
    "3. For manually marked exercises ensure you have completed any cells with `\"YOUR ANSWER HERE\"`.\n",
    "4. Ensure all cells are run with their output visible.\n",
    "5. Fill in your student ID (**only**) below.\n",
    "6. You should now **save and download** your work.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID:** 15006280"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_ai",
   "language": "python",
   "name": "adv_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
