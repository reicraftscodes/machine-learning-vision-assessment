{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff70665f9e181ee41198df9c4c362518",
     "grade": false,
     "grade_id": "cell-3557d234a08de81e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-danger\" style=\"color:black\"><b>Running ML-LV Jupyter Notebooks:</b><br>\n",
    "    <ol>\n",
    "        <li>Make sure you are running all notebooks using the <code>adv_ai</code> kernel.\n",
    "        <li><b>It is very important that you do not create any additional files within the weekly folders on CSCT cloud.</b> Any additional files, or editing the notebooks with a different environment may prevent submission/marking of your work.</li>\n",
    "            <ul>\n",
    "                <li>NBGrader will automatically fetch and create the correct folders files for you.</li>\n",
    "                <li>All files that are not the Jupyter notebooks should be stored in the 'ML-LV/data' directory.</li>\n",
    "            </ul>\n",
    "        <li>Please <b>do not pip install</b> any python packages (or anything else). You should not need to install anything to complete these notebooks other than the packages provided in the Jupyter CSCT Cloud environment.</li>\n",
    "    </ol>\n",
    "    <b>If you would like to run this notebook locally you should:</b><br>\n",
    "    <ol>\n",
    "        <li>Create an environment using the requirements.txt file provided. <b>Any additional packages you install will not be accessible when uploaded to the server and may prevent marking.</b></li>\n",
    "        <li>Download a copyÂ  of the notebook to your own machine. You can then edit the cells as you wish and then go back and copy the code into/edit the ones on the CSCT cloud in-place.</li>\n",
    "        <li><b>It is very important that you do not re-upload any notebooks that you have edited locally.</b> This is because NBGrader uses cell metadata to track marked tasks. <b>If you change this format it may prevent marking.</b></li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aa6dfdfbb764950cb88221f08909ac58",
     "grade": false,
     "grade_id": "cell-4931e253fb11d7e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Practical 9: Convolutional Neural Networks\n",
    "\n",
    "In the previous practical we applied several supervised and unsupervised algorithms to image classification for the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset. These algorithms perform remarkably well on the relatively simple digit images. However, you may have noticed they did not perform very well on your own handwritten digits. This is, at least in part, because these algorithms are highly sensitive to spatial variations within the image. Thus, these algorithms are quite brittle and will not perform well on more complex images.\n",
    "\n",
    "Convolutional Neural Networks (CNN) are the predominant network architecture for image classification and many other related tasks, such as object detection and image segmentation. They are inspired by the work of [Hubel, D. H. and Wiesel, T. N. (1959)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1363130/pdf/jphysiol01298-0128.pdf) on the receptive fields within a Cat's visual cortex. They noticed that certain (simple) cells only reacted to edges of a specific orientation, while more complex cells combined the outputs of simple cells and were able to respond to edges anywhere within the visual field. CNNs use these ideas in the form of *local receptive fields* and *shared weights* (convolution layers) and *spatial or temporal sub-sampling* (pooling layers). This makes them much more robust to variations within images, such as shifting patterns, occluded shapes and so on.\n",
    "\n",
    "In the first part of this practical we will examine what a convolution is and how they can be used as simple edge, or feature detectors.\n",
    "\n",
    "In the second part of this practical we will build and apply a simple CNN to classify the MNIST dataset, and our own handwritten digits.\n",
    "\n",
    "In the third part of this practical we will build a more complex CNN and use it to classify the flowers dataset.\n",
    "\n",
    "The objectives of this practical are:\n",
    "\n",
    "1. Understand the process of convolutions and the purpose of using different Kernels within CNNs\n",
    "\n",
    "2. Consider the role of convolutional layers and pooling layers within CNNs\n",
    "\n",
    "3. Apply a complete MV workflow for image classification with CNNs, including the use large pre-trained image classifiers for transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ead9f3913eec051e638df60b7fc6eec5",
     "grade": false,
     "grade_id": "cell-bb2eb2260cf4f4cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1 What is a Convolution\n",
    "\n",
    "For decades before the invention of 'neocognitrons' (nowadays better known as Convolutional Neural Networks),\n",
    "image processing experts relied on the concepts of transforming images by moving a small 'kernel' (also called a filter) across the image. The basic idea is to transform the image by using simple local processing which you apply at each different point in the image. Effectively a neighbourhood transform.\n",
    "\n",
    "To illustrate this process we will start by using a hand-drawn number (so we can process it quickly without code optimisations) and a simple vertical edge detector called the Prewitt Filter (technically the Prewitt uses vertical and horizontal versions then combines them).\n",
    "\n",
    "## 1.0 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress Tensorflow messages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.patches import Rectangle\n",
    "from IPython.display import HTML\n",
    "%matplotlib inline\n",
    "\n",
    "# Get the status of NBgrader (for skipping cell execution while validating/grading)\n",
    "grading = True if os.getenv('NBGRADER_EXECUTION') else False\n",
    "\n",
    "# Set seaborn style for matplotlib plots\n",
    "plt.style.use('seaborn-v0_8-white')\n",
    "\n",
    "# Get the project directory (should be in ML-LV)\n",
    "path = ''\n",
    "while os.path.basename(os.path.abspath(path)) != 'ML-LV':\n",
    "    path = os.path.abspath(os.path.join(path, '..'))\n",
    "\n",
    "# Set the directory to the datasets folder (should be in shared/datasets/mnist)\n",
    "datasets_dir = os.path.join(path, '..', 'shared', 'datasets', 'mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "87bdce54dec61c8daab88e32552f0a7f",
     "grade": false,
     "grade_id": "cell-c33b759cea8cb91b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1 Load digit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAJOCAYAAABBWYj1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAANlElEQVR4nO3dP4td9RqG4bVl1xkEESxE0Q8wRYSYQmMp2FtppVilkWglqFhY2QwWItpapYgGOzs7a0GIlZLCP0VAsQghus4HODnkPue82zWJ11UvHt6BKe75NbNb13VdAAC4qwe2PgAA4F4hnAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQLSvH+52u0PeAQCwmfqPVLw4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEC03/oAgDv5+OOPR/dee+210b1lWZYPP/xwdO/dd98d3btx48boHuDFCQAgE04AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiPZbHwDcH86dOze69+KLL47ures6urcsy3Lx4sXRvVu3bo3uvfHGG6N7gBcnAIBMOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAg2m99AHB/eOWVV0b3jo6ORvfuBbvdbusTgLvw4gQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAg2m99AHB3R0dHo3snJyeje8uyLC+99NL45j/NlStXtj4BuAsvTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQLTf+gC4Hx0dHY3uff7556N7Fy5cGN37p7p58+bo3o0bN0b3gHlenAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBE+60PgK09+eST45uffvrp6N6FCxdG95jxyy+/jO599913o3vAPC9OAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAtN/6APhvPf3006N7H3300ejesizL8fHx+OakL7/8cnzz559/Ht179dVXR/cO4erVq1ufAPzNvDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAADRfusDuL89/vjj45snJyeje8fHx6N7hzD9M7/zzjuje8uyLJcuXRrfPO1u3ry59QnA38yLEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBov/UBnC5nzpwZ3bt8+fLo3rIsy9mzZ8c3p7355pujeycnJ6N7t2/fHt0D+Kfw4gQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAET7rQ/gdPniiy9G986ePTu6dwgnJyenfvP27duje4fw+++/b30CwMF5cQIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAAKL91gfw/7l48eLo3jPPPDO6dy94/vnnxzcfeuih0b1vvvlmdO+rr74a3VuWZbl27dr4JsBp48UJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQLRb13VNH+52h76F/8Hrr78+uvfBBx+M7vm94X7266+/ju799NNPo3vXr18f3VuWZXn00UfHNyf9+eef45vvvffe6N7Vq1dH95gRc8iLEwBAJZwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEO3WdV3Th7vdoW/hFHjhhRdG95599tnRvWVZlocffnh077nnnhvdO4QHH3xwdO/MmTOje3A/++yzz0b3Xn755dE9ZsQc8uIEAFAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEu3Vd1/ThbnfoW4D/4LHHHhvde+SRR0b3lmVZjo6ORvfefvvt0b3z58+P7h3CH3/8Mbr322+/je7dC3744Yfxzbfeemt07+uvvx7dY0bMIS9OAACVcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAAKL91gcAd/fjjz+e6r1DOHfu3Oje+fPnR/cO4ZNPPhndu3Tp0uge4MUJACATTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCI9lsfAHAnt27d2voEgH/jxQkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIj2Wx8AcCdXrlwZ3Xv//fdH9w7hiSee2PoE4C68OAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCI9lsfAHAn67qO7v3111+je8uyLA88MPu351NPPTW6B8zz4gQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAET7rQ8AuJPvv/9+dO/bb78d3VuWZTk+Ph7dW9d1dA+Y58UJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQLTf+gCAv8P169fHN4+Pj0f3Ll++PLoHzPPiBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARLt1Xdf04W536FsAADYRc8iLEwBAJZwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCASDgBAETCCQAgEk4AAJFwAgCIhBMAQCScAAAi4QQAEAknAIBIOAEARMIJACASTgAAkXACAIiEEwBAJJwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAIuEEABAJJwCAaF8/XNf1kHcAAJx6XpwAACLhBAAQCScAgEg4AQBEwgkAIBJOAACRcAIAiIQTAEAknAAAon8BrDPVF5/1MXYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_val, y_val) = tf.keras.datasets.mnist.load_data(path=os.path.join(datasets_dir, 'mnist.npz'))\n",
    "\n",
    "# Randomly select an image of a specific digit\n",
    "digit = 4\n",
    "digit_indexes = np.where(y_train == digit)[0]\n",
    "rand_index = np.random.choice(digit_indexes, 1)[0]\n",
    "digit_img = X_train[rand_index]\n",
    "\n",
    "# Display digit\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.imshow(digit_img, cmap='gray')\n",
    "ax.axis('off')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd9c2e0e0af27af2e9e534ab14b8cf1b",
     "grade": false,
     "grade_id": "cell-94e2626dc7c615cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2 Define a kernel\n",
    "\n",
    "Create a simple vertical edge detector that looks at the average of the difference between the pixels on the left and those to  the right of where the kernel is on the image.\n",
    "\n",
    "First we will define a 3x3 kernel:\n",
    "\n",
    "$ k = \\begin{bmatrix}\n",
    "1 & 0 & -1\\\\\n",
    "1 & 0 & -1\\\\\n",
    "1 & 0 & -1\\\\\n",
    "\\end{bmatrix} $\n",
    "\n",
    "The kernel is then passed over the image and **convolved** (combined) with the pixels within the *sub-image* (the current 3x3 pixel values 'underneath' the kernel) to calculate a new value for the central pixel. At each step the pixels within the sub-image are multiplied by the *weight* in the corresponding position within the kernel. Then add up the result and set to the central pixel value in the new image.\n",
    "\n",
    "More formally:\n",
    "\n",
    "$ pixel[i] = \\sum_{j=1}^n k_j s[i]_j $\n",
    "\n",
    "Where $i$ is the current central pixel within the original image, $s[i]$ is the central pixel of the sub-image and $n = height \\times width $ of the kernel.\n",
    "\n",
    "![Illustration of convoltion from wikicommons](https://upload.wikimedia.org/wikipedia/commons/1/19/2D_Convolution_Animation.gif)\n",
    "\n",
    "**Padding**: Notice that since we put the result of our calculations into the pixel in the middle, we lose a border around the edge of the original image. With a 3x3 kernel it is not possible to calculate a new pixel value for the 1 pixel border around the image. For a 5x5 kernel the border would be 2 pixels. In CNNs this is usually accounted for by **padding** the image with 0's, which will not impact the output of the kernel and allow us to preserve all of the original pixel values. However, in this case the MNIST images already have a 4x4 border of 0's around the digit, so we wont lose any information.\n",
    "\n",
    "**Stride**: Here we are passing the kernel over the image 1 pixel at a time. However, sometimes, either for computational efficiency or because we wish to downsample, we move our kernel more than 1 pixel at a time, skipping the intermediate locations. This is particularly useful if the kernel is large, since it captures a larger area of the underlying image. The amount the kernel is moved at each step is called the **stride**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image shape: (28, 28)\n",
      "Kernel shape: (3, 3)\n",
      "Sub-image shape: (3, 3)\n",
      "Output image shape: (26, 26)\n"
     ]
    }
   ],
   "source": [
    "# Define the kernel\n",
    "kernel = np.array([[1, 0, -1],\n",
    "                   [1, 0, -1],\n",
    "                   [1, 0, -1]])\n",
    "\n",
    "kernel_height = kernel.shape[0]\n",
    "kernel_width = kernel.shape[1]\n",
    "\n",
    "# Calculate the border size to be lost\n",
    "border_x = kernel_height // 2\n",
    "border_y = kernel_width // 2\n",
    "\n",
    "# Calculate the output image size\n",
    "new_height = digit_img.shape[0] - 2 * border_x\n",
    "new_width = digit_img.shape[1] - 2 * border_y\n",
    "\n",
    "# Create the sub-image and output image\n",
    "sub_image = np.zeros((kernel_width, kernel_height), dtype='uint8')\n",
    "new_image = np.zeros((new_height, new_width), dtype='uint8')\n",
    "\n",
    "print(f\"Original image shape: {digit_img.shape}\")\n",
    "print(f\"Kernel shape: {kernel.shape}\")\n",
    "print(f\"Sub-image shape: {sub_image.shape}\")\n",
    "print(f\"Output image shape: {new_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ff67cd5d41ec08a59c7d219bde3d46a",
     "grade": false,
     "grade_id": "cell-19fa2ed80a108345",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.3 Perform convolution\n",
    "\n",
    "The following cell creates an animation of the convolution process. The convolution operation is performed within the `animate_convolution()` function.\n",
    "\n",
    "- The first plot is the original digit image with the current position of the kernel overlayed in yellow.\n",
    "\n",
    "- The middle plot shows the current sub-image, with the kernel weights overlayed. The result of the convolution operation is displayed at the top.\n",
    "\n",
    "- The final plot shows the resulting output image.\n",
    "\n",
    "<div class=\"alert alert-danger\" style=\"color:black\"><b>Warning:</b> This cell can take a while to run on the UWE machines/CSCT cloud!<br></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if not grading:\n",
    "#     # Plot the convolution process\n",
    "#     fig, ax = plt.subplots(1, 3, figsize=(8, 6))\n",
    "\n",
    "#     # Show original image\n",
    "#     ax[0].imshow(digit_img, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "#     # Add yellow outline of kernel position\n",
    "#     a0 = ax[0].add_patch(Rectangle([0, 0], kernel_width, kernel_height, fill=False, edgecolor='yellow', lw=1))\n",
    "\n",
    "#     # Plot the sub-image lying under the current kernel position\n",
    "#     a1 = ax[1].imshow(sub_image, cmap='gray', vmin=0, vmax=255)\n",
    "#     # Also show the corresponding kernel values in red\n",
    "#     for sub_x in range(kernel_width):\n",
    "#         for sub_y in range(kernel_height):\n",
    "#             text = ax[1].text(sub_y, sub_x, kernel[sub_x, sub_y], ha=\"left\", va=\"bottom\", color=\"r\")\n",
    "\n",
    "#     # Show the output image\n",
    "#     a2 = ax[2].imshow(new_image, cmap='gray', vmin=0, vmax=255)\n",
    "#     ax[2].set_title('Output Image')\n",
    "\n",
    "#     # Define the animation function\n",
    "#     def animate_convolution(i):\n",
    "\n",
    "#         # Get the current kernel position\n",
    "#         x = i % new_width\n",
    "#         y = i // new_width\n",
    "\n",
    "#         # Update the yellow outline of the kernel position\n",
    "#         a0.set(y=y - border_y, x=x - border_x)\n",
    "#         ax[0].set_title(f'Kernel Position: {a0.get_xy()}')\n",
    "\n",
    "#         # Update the sub-image\n",
    "#         sub_image = np.zeros((kernel_height, kernel_width), dtype=int)\n",
    "#         for sub_y in range(kernel_height):\n",
    "#             pixel_y = (y - border_y) + sub_y\n",
    "#             for sub_x in range(kernel_width):\n",
    "#                 pixel_x = (x - border_x) + sub_x \n",
    "#                 sub_image[sub_y][sub_x] = digit_img[pixel_y][pixel_x]\n",
    "\n",
    "#         # Update kernel display\n",
    "#         a1.set_data(sub_image)\n",
    "\n",
    "#         # Perform convolution\n",
    "#         result = kernel * sub_image\n",
    "#         new_pixel_val = np.sum(result)\n",
    "#         new_pixel_val = np.clip(new_pixel_val, 0, 255)\n",
    "#         ax[1].set_title(f'Computed value {new_pixel_val}')\n",
    "\n",
    "#         # Update the output image\n",
    "#         new_image[y][x] = int(new_pixel_val)\n",
    "#         a2.set_data(new_image)\n",
    "\n",
    "#     # Create the animation\n",
    "#     anim = FuncAnimation(fig, animate_convolution, frames=new_height*new_width-1, interval=50, repeat=True, repeat_delay=1000)\n",
    "#     html = HTML(anim.to_jshtml())\n",
    "#     display(html)\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f95e11dc753d86018868c7825650d25f",
     "grade": false,
     "grade_id": "cell-35a85a1768a6aaf1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color:black\"><h3>Before you submit this notebook to NBGrader for marking:</h3> \n",
    "\n",
    "1. Make sure have completed all exercises marked by <span style=\"color:blue\">**blue cells**</span>.\n",
    "2. For automatically marked exercises ensure you have completed any cells with `# YOUR CODE HERE`. Then click 'Validate' button above, or ensure all cells run without producing an error.\n",
    "3. For manually marked exercises ensure you have completed any cells with `\"YOUR ANSWER HERE\"`.\n",
    "4. Ensure all cells are run with their output visible.\n",
    "5. Fill in your student ID (**only**) below.\n",
    "6. You should now **save and download** your work.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID:** 15006280"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_ai",
   "language": "python",
   "name": "adv_ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
